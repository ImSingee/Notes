alias:: DDIA/ch5
- [link](https://github.com/Vonng/ddia/blob/master/ch5.md)
-
- 复制意味着在通过网络连接的多台机器上保留相同数据的副本
- 我们希望能复制数据，因为
	- 使得数据与用户在地理上接近（从而减少延迟）
	- 即使系统的一部分出现故障，系统也能继续工作（从而提高可用性）
	- 伸缩可以接受读请求的机器数量（从而提高读取吞吐量）
- **本章将假设你的数据集非常小，每台机器都可以保存整个数据集的副本**
	- 放开假设：[第六章 - 分片]([[DDIA/ch6]])
-
- 复制的困难之处在于处理复制数据的 **变更（change）**，我们将讨论三种流行的变更复制算法
	- **单领导者（single leader，单主）**
	- **多领导者（multi leader，多主）**
	- **无领导者（leaderless，无主）**
- 在复制时需要进行许多权衡：例如，使用同步复制还是异步复制？如何处理失败的副本？这些通常是数据库中的配置选项，细节因数据库而异，但原理在许多不同的实现中都类似。
-
- 分布式数据库
	- **最终一致性（eventual consistency）**
	- **读己之写（read-your-writes）**
	- **单调读（monotonic read）**
-
## 领导者与追随者
	- 存储了数据库拷贝的每个节点被称为 **副本（replica）**
	- 每一次向数据库的写入操作都需要传播到所有副本上，否则副本就会包含不一样的数据。最常见的解决方案被称为 **基于领导者的复制（leader-based replication）** （也称 **主动/被动（active/passive）** 复制或 **主/从（master/slave）** 复制）
		- 其中一个副本被指定为 **领导者（leader）**，也称为 **主库（master|primary）** 。当客户端要向数据库写入时，它必须将请求发送给该 **领导者**，其会将新数据写入其本地存储。
		- 其他副本被称为 **追随者（followers）**，亦称为 **只读副本（read replicas）**、**从库（slaves）**、**备库（ secondaries）** 或 **热备（hot-standby）**。每当领导者将新数据写入本地存储时，它也会将数据变更发送给所有的追随者，称之为 **复制日志（replication log）** 或 **变更流（change stream）**。每个跟随者从领导者拉取日志，并相应更新其本地数据库副本，方法是按照与领导者相同的处理顺序来进行所有写入。
		- 当客户想要从数据库中读取数据时，它可以向领导者或任一追随者进行查询。但**只有领导者才能接受写入操作**（从客户端的角度来看从库都是只读的）。
	- ### 同步复制与异步复制
		- 复制系统的一个重要细节是：复制是 **同步（synchronously）** 发生的还是 **异步（asynchronously）** 发生的。
			- 在关系型数据库中这通常是一个配置项，其他系统则通常硬编码为其中一个
		- 通常情况下，复制的速度相当快：大多数数据库系统能在不到一秒内完成从库的同步，但它们**不能提供复制用时的保证**。
			- 有些情况下，从库可能落后主库几分钟或更久，例如：从库正在从故障中恢复，系统正在最大容量附近运行，或者当节点间存在网络问题时。
		- 同步复制
			- 优点：从库能保证有与主库一致的最新数据副本。如果主库突然失效，我们可以确信这些数据仍然能在从库上找到。
			- 缺点：如果同步从库没有响应（比如它已经崩溃，或者出现网络故障，或其它任何原因），主库就无法处理写入操作。主库必须阻止所有写入，并等待同步副本再次可用。
			- 实际上，如果在数据库上启用同步复制，通常意味着其中 **一个** 从库是同步的，而其他的从库则是异步的。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库。 这种配置有时也被称为 **半同步（semi-synchronous）**
		- 异步复制
			- 通常情况下，基于领导者的复制都配置为完全异步。
			- 缺点：如果主库失效且不可恢复，则任何尚未复制给从库的写入都会丢失。这意味着即使已经向客户端确认成功，写入也不能保证是 **持久（Durable）** 的
			- 优点：即使所有的从库都落后了，主库也可以继续处理写入。
	- ### 设置新从库
		- 方案零（不行）：简单地将数据文件从一个节点复制到另一个节点通常是不够的
		- 方案一：锁定数据库（使其不可用于写入）来使磁盘上的文件保持一致
			- 这会违背高可用的目标
		- 方案二：基于快照
			- 将方案一的锁定数据库改为获得一个快照，将快照复制到新的从库节点
			- 从库连接到主库，并拉取快照之后发生的所有数据变更
				- 这要求快照与主库复制日志中的位置精确关联。该位置有不同的名称，例如 PostgreSQL 将其称为 **日志序列号（log sequence number，LSN）**，MySQL 将其称为 **二进制日志坐标（binlog coordinates）**
			- 当从库处理完快照之后积累的数据变更，我们就说它 **赶上（caught up）** 了主库，现在它可以继续及时处理主库产生的数据变化了。
	- ### 处理节点宕机
	  collapsed:: true
		- 我们的目标是，即使个别节点失效，也能保持整个系统运行，并尽可能控制节点停机带来的影响。
		- **从库失效：追赶恢复**
			- 如果从库崩溃并重新启动（或主库和从库之间的网络暂时中断）
				- 从库可以从日志中知道，在发生故障之前处理的最后一个事务。因此，从库可以连接到主库，并请求在从库断开期间发生的所有数据变更。当应用完所有这些变更后，它就赶上了主库，并可以像以前一样继续接收数据变更流。
		- **主库失效：故障切换**
			- 其中一个从库需要被提升为新的主库
				- 客户端写操作发送给新的主库
				- 其他从库需要开始拉取来自新主库的数据变更
			- 故障切换可以手动进行（通知管理员主库挂了，并采取必要的步骤来创建新的主库）或自动进行
				- 确认主库失效。
					- 有很多事情可能会出错：崩溃、停电、网络问题等等。
					- 没有万无一失的方法来检测出现了什么问题，所以大多数系统只是简单使用 **超时（Timeout）** ：节点频繁地相互来回传递消息，如果一个节点在一段时间内（例如 30 秒）没有响应，就认为它挂了（因为计划内维护而故意关闭主库不算）。
				- 选择一个新的主库
					- 可以通过选举过程（主库由剩余副本以多数选举产生）来完成
						- **共识** 问题（[第九章]([[DDIA/ch9]])）
					- 或者可以由之前选定的 **控制器节点（controller node）** 来指定新的主库
					- 注意：主库的最佳人选通常是拥有旧主库最新数据副本的从库（以最小化数据损失）
				- 重新配置相关系统以启用新的主库
					- 客户端 & 其他从库
			- **故障切换的过程中有很多地方可能出错**
				- 如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作
					- 在选出新主库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入
						- 最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对于数据持久性的期望。
				- 如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作
				- 发生某些故障时，可能会出现两个节点都以为自己是主库的情况。这种情况称为 **脑裂（split brain）**
					- 如果两个主库都可以接受写操作，却没有冲突解决机制，那么数据就可能丢失或损坏。
					- 一些系统采取了安全防范措施：当检测到两个主库节点同时存在时会关闭其中一个节点
						- 这种机制称为 **屏障（fencing）**，或者更充满感情的术语是：**爆彼之头（Shoot The Other Node In The Head, STONITH）**。
							- 将在 [第八章]([[DDIA/ch8]]) - 领导者和锁 中对屏障进行详细讨论。
						- 然而设计粗糙的机制可能最后会导致两个节点都被关闭
				- 主库被宣告死亡之前的正确超时应该怎么配置？
					- 超时时间越长意味着恢复时间也越长
					- 但是如果超时设置太短，又可能会出现不必要的故障切换
						- 临时的负载峰值可能导致节点的响应时间增加到超出超时时间
						- 网络故障也可能导致数据包延迟
						- 如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕
	- ### 复制日志的实现
		- **基于语句的复制**
			- 主库记录下它执行的每个写入请求（**语句**，即 statement）并将该语句日志发送给从库
				- 每个从库解析并执行该 SQL 语句，就像直接从客户端收到一样
			- 虽然听上去很合理，但有很多问题会搞砸这种复制方式
				- 任何调用 **非确定性函数（nondeterministic）** 的语句，可能会在每个副本上生成不同的值。例如，使用 `NOW()` 获取当前日期时间，或使用 `RAND()` 获取一个随机数。
				- 如果语句使用了 **自增列（auto increment）**，或者依赖于数据库中的现有数据（例如，`UPDATE ... WHERE <某些条件>`），则必须在每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制。
				- 有副作用的语句（例如：触发器、存储过程、用户定义的函数）可能会在每个副本上产生不同的副作用，除非副作用是绝对确定性的。
				- **有一定的办法绕开**
					- 当语句被记录时，主库可以用固定的返回值替换掉任何不确定的函数调用
				- **但是由于边缘情况实在太多了，现在通常会选择其他的复制方法。**
		- **传输预写式日志（WAL）**
			- [[DDIA/ch3：第三章：存储与检索]] 中，讨论了存储引擎如何在磁盘上表示数据，我们也发现了通常会将写操作追加到日志中
				- 对于日志结构存储引擎（请参阅 “[SSTables 和 LSM 树](((642521c8-fa76-4967-b1e6-5c9678ee0a07)))”），日志是主要的存储位置。日志段在后台压缩，并进行垃圾回收。
				- 对于覆写单个磁盘块的 [B 树](((642521c8-d65d-4715-8d86-565ed9d9cefb)))，每次修改都会先写入 **预写式日志（Write Ahead Log, WAL）**，以便崩溃后索引可以恢复到一个一致的状态。
			- 在任何一种情况下，该日志都是包含了所有数据库写入的仅追加字节序列。可以使用完全相同的日志在另一个节点上构建副本
				- 除了将日志写入磁盘之外，主库还可以通过网络将其发送给从库。
				- 通过使用这个日志，从库可以构建一个与主库一模一样的数据结构拷贝。
			- 缺点：日志记录的数据非常底层
				- WAL 包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。如果数据库将其存储格式从一个版本更改为另一个版本，通常不可能在主库和从库上运行不同版本的数据库软件。
					- 看上去这可能只是一个小的实现细节，但却可能对运维产生巨大的影响。如果复制协议允许从库使用比主库更新的软件版本，则可以先升级从库，然后执行故障切换，使升级后的节点之一成为新的主库，从而允许数据库软件的零停机升级。如果复制协议不允许版本不匹配（传输 WAL 经常出现这种情况），则此类升级需要停机。
			- 这种复制方法在 PostgreSQL 和 Oracle 等一些产品中被使用到
		- **逻辑日志复制（基于行）**
			- 思想类似于 WAL，但是**对复制和存储引擎使用不同的日志格式**
				- 可以将复制日志从存储引擎的内部实现中解耦出来
				- 这种复制日志被称为**逻辑日志（logical log）**，以将其与存储引擎的（物理）数据表示区分开来
			- 关系数据库的逻辑日志通常是以行的粒度来描述对数据库表的写入记录的序列
				- 对于插入的行，日志包含所有列的新值。
				- 对于删除的行，日志包含足够的信息来唯一标识被删除的行，这通常是主键，但如果表上没有主键，则需要记录所有列的旧值。
				- 对于更新的行，日志包含足够的信息来唯一标识被更新的行，以及所有列的新值（或至少所有已更改的列的新值）。
			- 修改多行的事务会生成多条这样的日志记录，后面跟着一条指明事务已经提交的记录。
			- 对于**外部应用程序**来说，逻辑日志格式也更容易解析。如果要将数据库的内容发送到外部系统，例如复制到数据仓库进行离线分析，或建立自定义索引和缓存，这一点会很有用。这种技术被称为 **数据变更捕获（change data capture）**
			- MySQL 的 binlog（当配置为使用基于行的复制时）使用了这种方法。
		- **基于触发器的复制**
			- 到目前为止描述的复制方法是由数据库系统实现的，不涉及任何应用程序代码。
			- 但在某些情况下需要更多的灵活性。
				- 例如，如果你只想复制数据的一个子集，或者想从一种数据库复制到另一种数据库，或者如果你需要冲突解决逻辑
				- 则可能需要将复制操作上移到应用程序层。
					- 一些工具，如 Oracle Golden Gate，可以通过读取数据库日志，使得其他应用程序可以使用数据
					- 另一种方法是使用许多关系数据库自带的功能：触发器和存储过程。
						- 触发器允许你将数据更改（写入事务）发生时自动执行的自定义应用程序代码注册在数据库系统中。触发器有机会将更改记录到一个单独的表中，使用外部程序读取这个表，再加上一些必要的业务逻辑，就可以将数据变更复制到另一个系统去
			- 基于触发器的复制通常比其他复制方法具有更高的开销，并且比数据库内置的复制更容易出错，也有很多限制。然而由于其灵活性，它仍然是很有用的。
## 复制延迟问题
	- 需要复制的原因
		- 容忍节点故障
		- 增强可伸缩性（处理比单个机器更多的请求）
		- 降低延迟（让副本在地理位置上更接近用户）
	- 基于领导者的复制要求所有写入都由单个节点处理，但只读查询可以由任何一个副本来处理。
	- **读伸缩（read-scaling）**
		- 对于读多写少的场景（Web 上的常见模式），一个有吸引力的选择是创建很多从库，并将读请求分散到所有的从库上去。这样能减小主库的负载，并允许由附近的副本来处理读请求
		- 只需添加更多的从库，就可以提高只读请求的服务容量
		- **只适用于异步复制**
			- 如果尝试同步复制到所有从库，则单个节点故障或网络中断将导致整个系统都无法写入
			- 节点越多越有可能出现个别节点宕机的情况，所以完全同步的配置将是非常不可靠的
			- **不幸的是，当应用程序从异步从库读取时，如果从库落后，它可能会看到过时的信息**
				- 对主库和从库执行相同的查询，可能得到不同的结果，因为并非所有的写入都反映在从库中
				- 这种不一致只是一个**暂时**的状态 —— 如果停止写入数据库并等待一段时间，从库最终会赶上并与主库保持一致。
				- 出于这个原因，这种效应被称为 **最终一致性（eventual consistency）**
					- 最终一致性中的 “最终” 一词有意进行了**模糊化**
						- 总的来说，**副本落后的程度是没有限制的**
						- 在正常的操作中，**复制延迟（replication lag）**，即写入主库到反映至从库之间的延迟，可能仅仅是几分之一秒，在实践中并不显眼
						- 但如果系统在接近极限的情况下运行，或网络中存在问题时，延迟可以轻而易举地超过几秒，甚至达到几分钟
	- ### 读己之写
		- 一个场景：A 服务器写，立刻 B 服务器读；异步复制场景下会感觉数据丢了一样、
		- 在这种情况下，我们需要 **写后读一致性（read-after-write consistency）**，也称为 **读己之写一致性（read-your-writes consistency）**
			- 如果用户重新加载页面，他们总会看到他们自己提交的任何更新
				- 它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到
		- 如何在基于领导者的复制系统中实现写后读一致性？
			- 对于用户 **可能修改过** 的内容，总是从主库读取
				- 社交网络上的用户个人资料信息通常只能由用户本人编辑，而不能由其他人编辑。因此一个简单的规则就是：总是从主库读取用户自己的档案，如果要读取其他用户的档案就去从库
			- 如果应用中的大部分内容都可能被用户编辑，使用其他标准来决定是否从主库读取。
				- 例如可以跟踪上次更新的时间，在上次更新后的一分钟内，从主库读；同时监控从库的复制延迟，防止向任何滞后主库超过一分钟的从库发出查询
			- 时间戳：客户端可以记住最近一次写入的时间戳，系统需要确保从库在处理该用户的读取请求时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读取，或者等待从库追赶上来
				- 这里的时间戳可以是逻辑时间戳（表示写入顺序的东西，例如日志序列号）或实际的系统时钟
					- 使用系统时钟需要考虑[时钟同步]([[DDIA/ch8]])
			- 如果你的副本分布在多个数据中心（为了在地理上接近用户或者出于可用性目的），还会有额外的复杂性：任何需要由主库提供服务的请求都必须路由到包含该主库的数据中心
		- 另一种复杂的情况发生在同一位用户从多个设备（例如桌面浏览器和移动 APP）请求服务的时候。这种情况下可能就需要提供跨设备的写后读一致性：如果用户在一个设备上输入了一些信息，然后在另一个设备上查看，则应该看到他们刚输入的信息。
			- 记住用户上次更新时间戳的方法变得更加困难，因为一个设备上运行的程序不知道另一个设备上发生了什么。需要对这些元数据进行中心化的存储
			- 如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。（例如，用户的台式计算机使用家庭宽带连接，而移动设备使用蜂窝数据网络，则设备的网络路由可能完全不同）。如果你的方法需要读主库，可能首先需要把来自该用户所有设备的请求都路由到同一个数据中心。
	- ### 单调读
		- **时光倒流（moving backward in time）**
			- 如果用户从不同从库进行多次读取，可能出现第一次读取存在的数据在第二次消失
		- **单调读（monotonic reads）**可以保证这种异常不会发生。这是一个比 **强一致性（strong consistency）** 更弱，但比 **最终一致性（eventual consistency）** 更强的保证
		- 实现单调读的一种方式是确保每个用户总是从同一个副本进行读取（不同的用户可以从不同的副本读取）。例如，可以基于用户 ID 的散列来选择副本，而不是随机选择副本。但是，如果该副本出现故障，用户的查询将需要重新路由到另一个副本。
	- ### 一致前缀读
		- **如果某些分区的复制速度慢于其他分区，那么观察者可能会在看到问题之前先看到答案**
		- **一致前缀读（consistent prefix reads）**：如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。
		- 这是 **分区（partitioned）** 或 **分片（sharded）** 数据库中的一个特殊问题
	- ### 复制延迟的解决方案
		- 在使用最终一致的系统时，如果复制延迟增加到几分钟甚至几小时，则应该考虑应用程序的行为
		- 应用程序可以提供比底层数据库更强有力的保证
			- 但在应用程序代码中处理这些问题是复杂的，容易出错
		- 分布式事务
## 多主复制
	- **多领导者配置**（multi-leader configuration，也称多主、多活复制，即 master-master replication 或 active/active replication）
		- 处理写入的每个节点都必须将该数据变更转发给所有其他节点（仍然是异步复制）
		- 在这种情况下，每个主库同时是其他主库的从库。
	- ### 多主复制的应用场景
	  collapsed:: true
		- 在单个数据中心内部使用多个主库的配置没有太大意义，因为其导致的复杂性已经超过了能带来的好处。但在一些情况下，这种配置也是合理的。
		- #### 运维多个数据中心
			- 多主配置中可以在每个数据中心都有主库。在每个数据中心内使用常规的主从复制；在数据中心之间，每个数据中心的主库都会将其更改复制到其他数据中心的主库中。
			- **性能**
				- 单主配置中，每个写入都必须穿过互联网，进入主库所在的数据中心。这可能会增加写入时间
				- 多主配置中，每个写操作都可以在本地数据中心进行处理，并与其他数据中心异步复制。因此，数据中心之间的网络延迟对用户来说是透明的，这意味着感觉到的性能可能会更好。
			- **容忍数据中心停机**
				- 在多主配置中，每个数据中心可以独立于其他数据中心继续运行，并且当发生故障的数据中心归队时，复制会自动赶上。
			- **容忍网络问题**
				- 单主配置对数据中心之间的连接问题非常敏感，因为通过这个连接进行的写操作是同步的。采用异步复制功能的多主配置通常能更好地承受网络问题：临时的网络中断并不会妨碍正在处理的写入。
			- **一个很大的缺点：两个不同的数据中心可能会同时修改相同的数据，写冲突是必须解决的**
			- 自增主键、触发器、完整性约束等都可能会有麻烦
				- 多主复制往往被认为是危险的领域，应尽可能避免
		- #### 需要离线操作的客户端
			- 多主复制的另一种适用场景是：应用程序在断网之后仍然需要继续工作。
			- 每个设备都有一个充当主库的本地数据库（它接受写请求），并且在所有设备上的日历副本之间同步时，存在异步的多主复制过程。复制延迟可能是几小时甚至几天，具体取决于何时可以访问互联网。
		- #### 协同编辑
	- ### 处理写入冲突
		- 多主复制的最大问题是可能发生**写冲突**，这意味着需要解决冲突。
		- #### 同步与异步冲突检测
			- 在多主配置中，两个写入都是成功的，在稍后的某个时间点才能异步地检测到冲突。那时再来要求用户解决冲突可能为时已晚。
			- 原则上，可以使冲突检测同步 - 即等待写入被复制到所有副本，然后再告诉用户写入成功。但是，通过这样做，你将失去多主复制的主要优点：允许每个副本独立地接受写入。如果你想要同步冲突检测，那么你可能不如直接使用单主复制。
		- #### 避免冲突
			- 处理冲突的最简单的策略就是避免它们
			- 例如，应用程序可以确保特定记录的所有写入都通过同一个主库
				- 在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心
				- 但从任何一位用户的角度来看，本质上就是单主配置了
			- 但是，有时你可能需要更改被指定的主库 —— 可能是因为某个数据中心出现故障，你需要将流量重新路由到另一个数据中心，或者可能是因为用户已经迁移到另一个位置，现在更接近其它的数据中心。在这种情况下，冲突避免将失效，你必须处理不同主库同时写入的可能性。
		- #### 收敛至一致的状态
			- 单主数据库按顺序进行写操作：如果同一个字段有多个更新，则最后一个写操作将决定该字段的最终值。
			- 在多主配置中，没有明确的写入顺序，所以最终值应该是什么并不清楚。
				- 如果每个副本只是按照它看到写入的顺序写入，那么数据库最终将处于不一致的状态
				- 数据库必须以一种 **收敛（convergent）** 的方式解决冲突，这意味着所有副本必须在所有变更复制完成时收敛至一个相同的最终值。
			- 实现冲突合并解决有多种途径：
				- 给每个写入一个唯一的 ID（例如时间戳、长随机数、UUID 或者键和值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。
					- 这种技术被称为 **最后写入胜利（LWW, last write wins）**。虽然这种方法很流行，但是很容易造成数据丢失
				- 为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级。这种方法也意味着数据丢失。
				- 以某种方式将这些值合并在一起 - 例如，按字母顺序排序，然后连接它们
				- 用一种可保留所有信息的显式数据结构来记录冲突，并编写解决冲突的应用程序代码（也许通过提示用户的方式）。
		- #### 自定义冲突解决逻辑
			- 解决冲突的最合适的方法可能取决于应用程序，大多数多主复制工具允许使用应用程序代码编写冲突解决逻辑。该代码可以在写入或读取时执行
				- **写时执行**：只要数据库系统检测到复制更改日志中存在冲突，就会调用冲突处理程序。例如，Bucardo 允许你为此编写一段 Perl 代码。这个处理程序通常不能提示用户 —— 它在后台进程中运行，并且必须快速执行。
				- **读时执行**：当检测到冲突时，所有冲突写入被存储。下一次读取数据时，会将这些多个版本的数据返回给应用程序。应用程序可以提示用户或自动解决冲突，并将结果写回数据库。例如 CouchDB 就以这种方式工作。
			- 冲突解决通常适用于单行记录或单个文档的层面，而不是整个事务
		- #### 自动冲突解决
			- 冲突解决规则可能很容易变得越来越复杂，自定义代码可能也很容易出错
			- 已经有一些有趣的研究来**自动解决**由于数据修改引起的冲突
				- **无冲突复制数据类型（Conflict-free replicated datatypes，CRDT）**是可以由多个用户同时编辑的集合、映射、有序列表、计数器等一系列数据结构，它们以合理的方式自动解决冲突。一些 CRDT 已经在 Riak 2.0 中实现。
				- **可合并的持久数据结构（Mergeable persistent data structures）**显式跟踪历史记录，类似于 Git 版本控制系统，并使用三向合并功能（而 CRDT 使用双向合并）。
				- **操作转换（operational transformation）**是 Etherpad  和 Google Docs  等协同编辑应用背后的冲突解决算法。它是专为有序列表的并发编辑而设计的，例如构成文本文档的字符列表。
	- ### 多主复制拓扑
		- **复制拓扑**（replication topology）用来描述写入操作从一个节点传播到另一个节点的通信路径
		- 最常见的拓扑是全部到全部 all-to-all
			- 每个主库都将其写入发送给其他所有的主库
		- 一些更受限的拓扑也会被使用到：例如，默认情况下 MySQL 仅支持 **环形拓扑（circular topology）**
			- 每个节点都从一个节点接收写入，并将这些写入（加上自己的写入）转发给另一个节点
		- 另一种流行的拓扑结构具有星形的形状：一个指定的根节点将写入转发给所有其他节点
			- 星形拓扑可以推广到树
		- 在环形和星形拓扑中，写入可能需要在到达所有副本之前通过多个节点。因此，节点需要转发从其他节点收到的数据更改
			- 为了防止无限复制循环，每个节点被赋予一个唯一的标识符，并且在复制日志中，每次写入都会使用其经过的所有节点的标识符进行标记
			- 当一个节点收到用自己的标识符标记的数据更改时，该数据更改将被忽略，因为节点知道它已经被处理过
		- 环形和星形拓扑的问题是，如果只有一个节点发生故障，则可能会中断其他节点之间的复制消息流，导致它们无法通信，除非节点被修复
		- 全部到全部的拓扑也可能有问题。特别是，一些网络链接可能比其他网络链接更快（例如由于网络拥塞），结果是一些复制消息可能 “超越” 其他复制消息
		- 要正确排序这些事件，可以使用一种称为 **版本向量（version vectors）** 的技术
## 无主复制
	- 单主复制、多主复制的思路：
		- 客户端向一个主库发送写请求，而数据库系统负责将写入复制到其他副本。主库决定写入的顺序，而从库按相同顺序应用主库的写入。
	- **无主的（leaderless）**：一些数据存储系统采用不同的方法，放弃主库的概念，并允许任何副本直接接受来自客户端的写入
		- 在关系数据库主导的时代，这个想法几乎已被忘却
		- 亚马逊将其用于其内部的 Dynamo 系统
		- Riak，Cassandra 和 Voldemort 是受 Dynamo 启发的无主复制模型的开源数据存储，所以这类数据库也被称为 *Dynamo 风格*。
	- 实现
		- 一：客户端直接将写入发送到几个副本中
		- 二：由一个 **协调者（coordinator）** 节点代表客户端进行写入
			- 与主库数据库不同，协调者不执行特定的写入顺序
	- ### 当节点故障时写入数据库
	  collapsed:: true
		- 假设你有一个带有三个副本的数据库，而其中一个副本目前不可用
			- 在基于领导者的配置中，如果要继续处理写入，则可能需要执行故障切换
			- **在无主配置中，不存在故障转移**
		- 读写数据
			- 写：客户端并行发送写入到所有副本，当用户收到*多个*成功的响应后，认为写入成功；客户简单地忽略了某些副本可能错过了写入的事实
			- 读：读请求将被并行地发送到所有节点。客户可能会从*多个*节点获得响应，这些响应可能不同，即来自一个节点的最新值和来自另一个节点的陈旧值。版本号将被用于确定哪个值是更新的
			- > 具体所谓的「多个」到底是多少请参考 [读写的法定人数](((64378d44-d345-4df0-8277-af0b5ca7a35c)))
		- #### 读修复和反熵
			- **复制方案应确保最终将所有数据复制到每个副本**
			- 在一个不可用的节点重新联机之后，它如何赶上它错过的写入？
				- **读修复（Read repair）**
					- 当客户端并行读取多个节点时，它可以检测到任何陈旧的响应，客户端发现陈旧值时，将新值写回到该副本
					- 这种方法适用于读频繁的值
				- **反熵过程（Anti-entropy process）**
					- 一些数据存储具有后台进程，该进程不断查找副本之间的数据差异，并将任何缺少的数据从一个副本复制到另一个副本
					- 与基于领导者的复制中的复制日志不同，此反熵过程不会以任何特定的顺序复制写入，并且在复制数据之前可能会有显著的延迟。
			- 并不是所有的系统都实现了这两种机制
				- 请注意，如果没有反熵过程，很少被读取的值可能会从某些副本中丢失，从而降低了持久性，因为只有在应用程序读取值时才执行读修复
		- #### 读写的法定人数
		  id:: 64378d44-d345-4df0-8277-af0b5ca7a35c
			- 如果有 n 个副本，每个写入必须由 w 个节点确认才能被认为是成功的，并且我们必须至少为每个读取查询 r 个节点
				- w + r > n 我们可以预期在读取时能获得最新的值，因为 r 个读取中至少有一个节点是最新的
				- 遵循这些 r 值和 w 值的读写称为 **法定人数（quorum）**的读和写
				- r 和 w 是有效读写所需的最低票数
				- 集群中可能有多于 n 个的节点（集群的机器数可能多于副本数目）。但是任何给定的值只能存储在 n 个节点上
			- 一个常见的选择是使 n 为奇数（通常为 3 或 5）并设置 w = r = (n+1)/2
				- 在 Dynamo 风格的数据库中，参数 n、w 和 r 通常是可配置的
				- 修改：写入次数较少且读取次数较多的工作负载可以从设置 w=n & r=1
					- 这会使得读取速度更快，但缺点是只要有一个不可用的节点就会导致所有的数据库写入都失败
			- 容忍情况
				- 如果 w < n，当节点不可用时，我们仍然可以处理写入。
				- 如果 r < n，当节点不可用时，我们仍然可以处理读取。
				- 我们可以容忍 min(m-r, n-2) 个不可用的节点
				- 通常，读取和写入操作始终并行发送到所有 n 个副本
					- 参数 w 和 r 决定我们等待多少个节点，即在我们认为读或写成功之前，有多少个节点需要报告成功
					- 如果可用的节点少于所需的 w 或 r，则写入或读取将返回错误
						- 节点可能由于多种原因而不可用
							- 比如：节点关闭（异常崩溃，电源关闭）、操作执行过程中的错误（由于磁盘已满而无法写入）、客户端和服务器节点之间的网络中断或任何其他原因
						- 我们只需要关心节点是否返回了成功的响应，而不需要区分不同类型的错误
	- ### 法定人数一致性的局限性
		- **法定人数不一定必须是大多数，重要的是读写使用的节点至少有一个节点的交集**。其他法定人数的配置是可能的，这使得分布式算法的设计有一定的灵活性
			- 甚至可以 w+r <= n（即法定条件不满足）
				- 在这种情况下，读取和写入操作仍将被发送到 n 个节点，但操作成功只需要少量的成功响应。
				- 较小的 w 和 r 更有可能会读取到陈旧的数据（因为你的读取更有可能未包含具有最新值的节点）
				- 另一方面，这种配置允许更低的延迟和更高的可用性
		- 然而，在 w+r > n 的情况下（即满足法定条件），也存在 **边缘情况** 可能会返回陈旧值
			- 使用[宽松的法定人数](((64378f63-5e58-443d-93c1-07d415ad5338)))时，，w 个写入和 r 个读取有可能落在完全不同的节点上
			- 如果两个写入同时发生，不清楚哪一个先发生。在这种情况下，唯一安全的解决方案是合并并发写入
			- 如果写操作与读操作同时发生，写操作可能仅反映在某些副本上。在这种情况下，不确定读取返回的是旧值还是新值
			- 如果写操作在某些副本上成功，而在其他节点上失败（例如，因为某些节点上的磁盘已满），在小于 w 个副本上写入成功。所以整体判定写入失败，但**整体写入失败并没有在写入成功的副本上回滚**。这意味着一个写入虽然报告失败，后续的读取仍然可能会读取这次失败写入的值
			- 如果携带新值的节点发生故障，需要从其他带有旧值的副本进行恢复，则存储新值的副本数可能会低于 w，从而打破法定人数条件
			- 使一切工作正常，有时也会不幸地出现关于 **时序（timing）** 的边缘情况
		- 因此，**你可以通过参数 w 和 r 来调整读取到陈旧值的概率，但把它们当成绝对的保证是不明智的**
		- #### 监控陈旧度
			- 从运维的角度来看，监视你的数据库是否返回最新的结果是很重要的。即使应用可以容忍陈旧的读取，你也需要了解复制的健康状况
			- 已经有一些关于衡量无主复制数据库中的复制陈旧度的研究，并根据参数 n、w 和 r 来预测陈旧读取的预期百分比。不幸的是，这还不是很常见的做法，但是将陈旧测量值包含在数据库的标准度量集中是一件好事。虽然最终一致性是一种有意模糊的保证，但是从可操作性角度来说，能够量化 “最终” 也是很重要的。
	- ### 宽松的法定人数与提示移交
	  id:: 64378f63-5e58-443d-93c1-07d415ad5338
		- 在一个大型的集群中（节点数量明显多于 n 个），网络中断期间客户端可能仍能连接到一些数据库节点，但又不足以组成一个特定的法定人数。在这种情况下，数据库设计人员需要权衡一下：
			- 对于所有无法达到 w 或 r 个节点法定人数的请求，是否返回错误是更好的？
			- **宽松的法定人数（sloppy quorum）**：或者我们是否应该接受写入，然后将它们写入一些可达的节点，但不在这些值通常所存在的 n 个节点上？
				- 再次提示：n 只是副本数，可能小于总节点数
		- 在宽松的法定人数下
			- 写和读仍然需要 w 和 r 个成功的响应，但这些响应可能来自不在指定的 n 个 “主” 节点中的其它节点
			- 一旦网络中断得到解决，一个节点代表另一个节点临时接受的任何写入都将被发送到适当的 “主” 节点。这就是所谓的 **提示移交（hinted handoff）**
			- 宽松的法定人数对写入可用性的提高特别有用：只要有任何 w 个节点可用，数据库就可以接受写入。然而，这意味着即使当 w+r > n 时，也不能确保读取到某个键的最新值，因为最新的值可能已经临时写入了 n 之外的某些节点
		- **在传统意义上，宽松的法定人数实际上并不是法定人数。它只是一个持久性的保证，即数据已存储在某处的 w 个节点。但不能保证 r 个节点的读取能看到它，除非提示移交已经完成。**
		- 在所有常见的 Dynamo 实现中，宽松的法定人数是可选的。
			- 在 Riak 中，它们默认是启用的
			- 而在 Cassandra 和 Voldemort 中它们默认是禁用的
		- #### 运维多个数据中心
			- 主复制也适用于多数据中心操作，既然它旨在容忍冲突的并发写入、网络中断和延迟尖峰
			- Cassandra 和 Voldemort 在正常的无主模型中实现了他们的多数据中心支持：副本的数量 n 包括所有数据中心的节点，你可以在配置中指定每个数据中心所拥有的副本的数量。无论数据中心如何，每个来自客户端的写入都会发送到所有副本，但客户端通常只等待来自其本地数据中心内的法定节点的确认，从而不会受到跨数据中心链路延迟和中断的影响。对其他数据中心的高延迟写入通常被配置为异步执行，尽管该配置仍有一定的灵活性
			- Riak 将客户端和数据库节点之间的所有通信保持在一个本地的数据中心，因此 n 描述了一个数据中心内的副本数量。数据库集群之间的跨数据中心复制在后台异步发生，其风格类似于多主复制
	- ### 检测并发写入
		- Dynamo 风格的数据库允许多个客户端同时写入相同的键（Key），这意味着即使使用严格的法定人数也会发生冲突。
			- 这种情况与多主复制相似
			- 但在 Dynamo 风格的数据库中，在 **读修复** 或 **提示移交** 期间也可能会产生冲突
		- 其问题在于，由于可变的网络延迟和部分节点的故障，事件可能以不同的顺序到达不同的节点
		- **如果每个节点只要接收到来自客户端的写入请求就简单地覆写某个键值，那么节点就会永久地不一致**
		- 为了最终达成一致，副本应该趋于相同的值。如何做到这一点？
			- 有人可能希望复制的数据库能够自动处理，但不幸的是，大多数的实现都很糟糕：如果你想避免丢失数据，你（应用程序开发人员）需要知道很多有关数据库冲突处理的内部信息。
		- #### 最后写入胜利（丢弃并发写入）
			- 实现最终收敛的一种方法是声明每个副本只需要存储 **“最近”** 的值，并允许 **“更旧”** 的值被覆盖和抛弃。然后，只要我们有一种明确的方式来确定哪个写是 “最近的”，并且每个写入最终都被复制到每个副本，那么复制最终会收敛到相同的值。
				- *「最近」的误导性*：当客户端向数据库节点发送写入请求时，两个客户端都不知道另一个客户端，因此不清楚哪一个先发送请求
					- 事实上，说这两种情况谁先发送请求是没有意义的：既然我们说写入是 **并发（concurrent）** 的，那么它们的顺序就是不确定的。
			- **即使写入没有自然的排序，我们也可以强制进行排序**
				- 可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 **“最近的”**，并丢弃具有较早时间戳的任何写入。这种冲突解决算法被称为 **最后写入胜利（LWW, last write wins）**
					- 是 Cassandra 唯一支持的冲突解决方法，也是 Riak 中的一个可选特征
				- LWW 实现了最终收敛的目标，但以 **持久性** 为代价：如果同一个键有多个并发写入，即使它们反馈给客户端的结果都是成功的（因为它们被写入 w 个副本），也只有一个写入将被保留，而其他写入将被默默地丢弃。
					- 此外，LWW 甚至可能会丢弃不是并发的写入，我们将在 [有序事件的时间戳]([[DDIA/ch8]]) 中进行讨论。
			- **在类似缓存的一些情况下，写入丢失可能是可以接受的。但如果数据丢失不可接受，LWW 是解决冲突的一个很烂的选择。**
			- 在数据库中使用 LWW 的唯一安全方法是确保一个键只写入一次，然后视为不可变，从而避免对同一个键进行并发更新
			- #### “此前发生”的关系和并发
				- 我们如何判断两个操作是否是并发的？
					- 如果操作 B 了解操作 A，或者依赖于 A，或者以某种方式构建于操作 A 之上，则操作 A 在操作 B 之前发生（**happens before**）
					- 如果两个操作中的任何一个都不在另一个之前发生（即，两个操作都不知道对方），那么这两个操作是并发的
					- **只要有两个操作 A 和 B，就有三种可能性：A 在 B 之前发生，或者 B 在 A 之前发生，或者 A 和 B 并发**
				- 我们需要的是一个算法来告诉我们两个操作是否是并发的；然后，如果是并发的，需要解决冲突
				- **并发性、时间和相对性**
					- 如果两个操作 **“同时”** 发生，似乎应该称为并发 —— 但事实上，它们在字面时间上重叠与否并不重要。由于分布式系统中的时钟问题，现实中是很难判断两个事件是否是 **同时** 发生的
					- 为了定义并发性，确切的时间并不重要：如果两个操作都意识不到对方的存在，就称这两个操作 **并发**，而不管它们实际发生的物理时间
			- #### 捕获"此前发生"关系
				- **核心：每次操作时带上上一次的版本号**
					- 服务器可以只通过查看版本号来确定两个操作是否是并发的 —— 它不需要对值本身进行解释
				- 流程
					- 服务器为每个键维护一个版本号，每次写入该键时都递增版本号，并将新版本号与写入的值一起存储。
					- 当客户端读取键时，服务器将返回所有未覆盖的值以及最新的版本号。客户端在写入前必须先读取。
					- 当客户端写入键时，必须包含之前读取的版本号，并且必须将之前读取的所有值合并在一起（针对写入请求的响应可以像读取请求一样，返回所有当前值，这使得我们可以像购物车示例那样将多个写入串联起来）。
					- 当服务器接收到具有特定版本号的写入时，它可以覆盖该版本号或更低版本的所有值（因为它知道它们已经被合并到新的值中），但是它必须用更高的版本号来保存所有值（因为这些值与正在进行的其它写入是并发的）。
					- *当一个写入包含前一次读取的版本号时，它会告诉我们的写入是基于之前的哪一种状态。如果在不包含版本号的情况下进行写操作，则与所有其他写操作并发，因此它不会覆盖任何内容 —— 只会在随后的读取中作为其中一个值返回*
			- #### 合并并发写入的值
				- 这种算法可以确保没有数据被无声地丢弃，但不幸的是，客户端需要做一些额外的工作：客户端随后必须合并并发写入的值。 Riak 称这些并发值为 **兄弟（siblings）**
				- 一种合理的合并值的方法就是做并集
					- 然而，如果想删除，那么把并发值做并集可能不会产生正确的结果
					- 因此，*要移除一个项目时不能简单地直接从数据库中删除*；相反，系统必须留下一个具有适当版本号的标记，以在兄弟合并时表明该项目已被移除。这种删除标记被称为 **墓碑（tombstone）**
				- **因为在应用程序代码中做兄弟合并是复杂且容易出错的，所以有一些数据结构被设计出来用于自动执行这种合并**
					- CRDT
			- #### 版本向量
				- 当多个副本并发接受写入时，使用单个版本号是不够
					- 单个版本号只适用于一个副本的情况
					- 除了对每个键，我们还需要对 **每个副本** 使用版本号
						- 每个副本在处理写入时增加自己的版本号，并且跟踪从其他副本中看到的版本号
				- 所有副本的版本号集合称为 **版本向量（version vector）**
					- 版本向量有时也被称为向量时钟，即使它们*不完全相同*
						- Jonathan Ellis: “[Why Cassandra Doesn't Need Vector Clocks](http://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks),” *datastax.com*, September 2, 2013.
						- Russell Brown: “[Vector Clocks Revisited Part 2: Dotted Version Vectors](http://basho.com/posts/technical/vector-clocks-revisited-part-2-dotted-version-vectors/),” *basho.com*, November 10, 2015.
						- Carlos Baquero: “[Version Vectors Are Not Vector Clocks](https://haslab.wordpress.com/2011/07/08/version-vectors-are-not-vector-clocks/),” *haslab.wordpress.com*, July 8, 2011.
				- 当读取值时，版本向量会从数据库副本发送到客户端，并且随后写入值时需要将其发送回数据库
					- 版本向量允许数据库区分覆盖写入和并发写入
					- 版本向量结构能够确保从一个副本读取并随后写回到另一个副本是安全的
						- 这样做虽然可能会在其他副本上面创建数据，但只要能正确合并就不会丢失数据
## 本章小结
	- 复制可以用于几个目的
		- 高可用性
		- 断开连接的操作
		- 延迟
		- 可伸缩性
	- 复制的三种主要方法
		- 单主复制
		- 多主复制
		- 无主复制
	- 应用程序在复制延迟时的行为的一致性模型
		- 写后读一致性
		- 单调读
		- 一致前缀读
		-