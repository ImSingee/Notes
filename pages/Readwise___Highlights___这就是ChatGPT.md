title:: Readwise/Highlights/这就是ChatGPT
author:: [[斯蒂芬·沃尔弗拉姆]]
full-title:: 这就是ChatGPT
category:: #books


![](https://cdn.weread.qq.com/weread/cover/24/cpplatform_4cn8w4tmgzntjobg9ffeny/s_cpplatform_4cn8w4tmgzntjobg9ffeny1703648785.jpg)
None

- ChatGPT的基础是人工神经网络（本书中一般简称为神经网络或网络），后者最初是在20世纪40年代为了模拟理想化的大脑运作方式而发明的 #Highlight #[[2024-04-26]]
- GPT技术路线的一大核心理念，是用最简单的自回归生成架构来解决无监督学习问题，也就是利用无须人特意标注的原始数据，学习其中对世界的映射 #Highlight #[[2024-04-26]]
- 神经网络之所以很有用（人脑中的神经网络大概也如此），原因不仅在于它可以执行各种任务，还在于它可以通过逐步“根据样例训练”来学习执行这些任务。 #Highlight #[[2024-05-20]]
- 从来没有“无模型的模型”。你使用的任何模型都有某种特定的基本结构，以及用于拟合数据的一定数量的“旋钮”（也就是可以设置的参数）。ChatGPT使用了许多这样的“旋钮”—实际上有1750亿个。 #Highlight #[[2024-05-20]]
- 最佳思路是建立一个模型，让我们能够估计序列出现的概率—即使我们从未在已有的文本语料库中明确看到过这些序列。ChatGPT的核心正是所谓的“大语言模型”，后者已经被构建得能够很好地估计这些概率了。 #Highlight #[[2024-05-20]]
- 上图展示了，在仅有两个权重的情况下可能需要进行的最小化工作。但是事实证明，即使有更多的权重（ChatGPT使用了1750亿个权重），也仍然可以进行最小化，至少可以在某种程度上进行近似。实际上，“深度学习”在2012年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。 #Highlight #[[2024-05-20]]
- 有时候用神经网络解决复杂问题比解决简单问题更容易—这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。 #Highlight #[[2024-05-20]]
- 在典型情况下，有许多不同的权重集合可以使神经网络具有几乎相同的性能。在实际的神经网络训练中，通常会做出许多随机选择，导致产生一些“不同但等效”的解决方案 #Highlight #[[2024-05-20]]
- 我们能“从数学上”解释网络是如何做出区分的吗？并不能。它只是在“做神经网络要做的事”。但是事实证明，这通常与我们人类所做的区分相当吻合 #Highlight #[[2024-05-20]]
- 人类大脑有大约1000亿个神经元（神经细胞），每个神经元都能够产生电脉冲，最高可达每秒约1000次。这些神经元连接成复杂的网络，每个神经元都有树枝状的分支，从而能够向其他数千个神经元传递电信号。粗略地说，任意一个神经元在某个时刻是否产生电脉冲，取决于它从其他神经元接收到的电脉冲，而且神经元不同的连接方式会有不同的“权重”贡献。 #Highlight #[[2024-05-28]]
- 重点在于，已训练的神经网络能够对所展示的特定例子进行“泛化” #Highlight #[[2024-05-28]]
- 无论输入什么，神经网络都会生成一个答案。结果表明，它的做法相当符合人类的思维方式。正如上面所说的，这并不是我们可以“根据第一性原则推导”出来的事实。这只是一些经验性的发现，至少在某些领域是正确的。但这是神经网络有用的一个关键原因：它们以某种方式捕捉了“类似人类”的做事方式。 #Highlight #[[2024-05-28]]