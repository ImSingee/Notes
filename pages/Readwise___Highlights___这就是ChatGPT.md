title:: Readwise/Highlights/这就是ChatGPT
author:: [[斯蒂芬·沃尔弗拉姆]]
full-title:: 这就是ChatGPT
category:: #books


![](https://cdn.weread.qq.com/weread/cover/24/cpplatform_4cn8w4tmgzntjobg9ffeny/s_cpplatform_4cn8w4tmgzntjobg9ffeny1703648785.jpg)
None

- ChatGPT的基础是人工神经网络（本书中一般简称为神经网络或网络），后者最初是在20世纪40年代为了模拟理想化的大脑运作方式而发明的 #Highlight #[[2024-04-26]]
- GPT技术路线的一大核心理念，是用最简单的自回归生成架构来解决无监督学习问题，也就是利用无须人特意标注的原始数据，学习其中对世界的映射 #Highlight #[[2024-04-26]]
- 神经网络之所以很有用（人脑中的神经网络大概也如此），原因不仅在于它可以执行各种任务，还在于它可以通过逐步“根据样例训练”来学习执行这些任务。 #Highlight #[[2024-05-20]]
- 从来没有“无模型的模型”。你使用的任何模型都有某种特定的基本结构，以及用于拟合数据的一定数量的“旋钮”（也就是可以设置的参数）。ChatGPT使用了许多这样的“旋钮”—实际上有1750亿个。 #Highlight #[[2024-05-20]]
- 最佳思路是建立一个模型，让我们能够估计序列出现的概率—即使我们从未在已有的文本语料库中明确看到过这些序列。ChatGPT的核心正是所谓的“大语言模型”，后者已经被构建得能够很好地估计这些概率了。 #Highlight #[[2024-05-20]]
- 上图展示了，在仅有两个权重的情况下可能需要进行的最小化工作。但是事实证明，即使有更多的权重（ChatGPT使用了1750亿个权重），也仍然可以进行最小化，至少可以在某种程度上进行近似。实际上，“深度学习”在2012年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。 #Highlight #[[2024-05-20]]
- 有时候用神经网络解决复杂问题比解决简单问题更容易—这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。 #Highlight #[[2024-05-20]]
- 在典型情况下，有许多不同的权重集合可以使神经网络具有几乎相同的性能。在实际的神经网络训练中，通常会做出许多随机选择，导致产生一些“不同但等效”的解决方案 #Highlight #[[2024-05-20]]
- 我们能“从数学上”解释网络是如何做出区分的吗？并不能。它只是在“做神经网络要做的事”。但是事实证明，这通常与我们人类所做的区分相当吻合 #Highlight #[[2024-05-20]]