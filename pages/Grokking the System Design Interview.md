- [课程链接](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview)
-
- ## System Design Guide
  collapsed:: true
	- ### System Design Interviews: A step by step guide
	  collapsed:: true
		- **Step 1: Requirements clarifications**
			- It is always a good idea to ask questions about the exact scope of the problem we are trying to solve. Candidates who spend enough time to define the end goals of the system always have a better chance to be successful in the interview.
		- **Step 2: Back-of-the-envelope estimation**
			- It is always a good idea to estimate the scale of the system we’re going to design. This will also help later when we focus on scaling, partitioning, load balancing, and caching.
			- scale, storage, network bandwidth
		- **Step 3: System interface definition**
			- Define what APIs are expected from the system. This will establish the exact contract expected from the system and ensure if we haven’t gotten any requirements wrong.
		- **Step 4: Defining data model**
			- Defining the data model in the early part of the interview will clarify how data will flow between different system components.
			- Later, it will guide for data partitioning and management.
				- storage, transportation, encryption
		- **Step 5: High-level design**
			- Draw a block diagram with **5-6 boxes** representing the core components of our system. We should identify enough components that are needed to solve the actual problem from end to end.
		- **Step 6: Detailed design**
			- Dig deeper into two or three major components; the interviewer’s feedback should always guide us to what parts of the system need further discussion.
			- We should present different approaches, their pros and cons, and explain why we will prefer one approach over the other.
				- Remember, there is no single answer; the only important thing is to consider tradeoffs between different options while keeping system constraints in mind.
		- **Step 7: Identifying and resolving bottlenecks**
			- Try to discuss as many bottlenecks as possible and different approaches to mitigate them.
				- single point of failure
				- replicas of the data
				- enough copies of different services
				- monitoring the performance, get alerts whenever critical components fail or their performance degrades
		- Preparation and being organized during the interview are the keys to success in system design interviews.
	- ### Designing a URL Shortening service like TinyURL
	  collapsed:: true
		- **1. Why do we need URL shortening?**
		- **2. Requirements and Goals of the System**
			- You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind.
			- Functional Requirements, Non-Functional Requirements, Extended Requirements
		- **3. Capacity Estimation and Constraints**
			- **Traffic estimates**, **Storage estimates**, **Bandwidth estimates**, **Memory estimates**
			- **High-level estimates** (Summary)
		- **4. System APIs**
			- Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system.
			- **How do we detect and prevent abuse?**
		- **5. Database Design**
			- Defining the DB schema in the early stages of the interview would help to understand the data flow among various components and later would guide towards data partitioning.
			- **Database Schema**
			- **What kind of database should we use?**
				- [SQL vs. NoSQL](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview&unit=grokking-the-system-design-interview_1627054379423_8Unit)
		- **6. Basic System Design and Algorithm**
			- Hash or **Key Generation Service (KGS)**
		- **7. Data Partitioning and Replication**
			- **Range Based Partitioning** or **Hash-Based Partitioning**
				- [Consistent Hashing](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview&unit=grokking-the-system-design-interview_1627054411532_11Unit)
		- **8. Cache**
			- **cache memory**, **cache eviction policy **
		- **9. Load Balancer (LB)**
		- **10. Purging or DB cleanup**
		- **11. Telemetry**
		- **12. Security and Permissions**
			- permission level (public/private)
- ## System Design Problems
	- ### Designing Pastebin
	  collapsed:: true
		- We can assume a 5:1 ratio between the read and write.
		- Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs, or logs. Such texts are not huge, so let’s assume that each paste on average contains 10KB.
		- To keep some margin, we will assume a 70% capacity model (meaning we don’t want to use more than 70% of our total storage capacity at any point),
		- Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes.
		- ‘ContentKey’ is a reference to an external object storing the contents of the paste
		- Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don’t see failure due to the duplicate key.
	- ### Designing Instagram
	  collapsed:: true
		- High Availability
		- The acceptable latency of the system is 200ms for News Feed generation.
		- We may need to use SQL database, but relational databases come with their challenges, especially when we need to scale them.
		- In data stores, deletes don’t get applied instantly; data is retained for certain days (to support undeleting) before getting removed from the system permanently.
		- We can split reads and writes into separate services. We will have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system.
		- Losing files is not an option for our service. Therefore, we will store multiple copies of each file so that if one storage server dies, we can retrieve the photo from the other copy present on a different storage server.
		- KGS single point of failure: A workaround for that could be to define two such databases, one generating even-numbered IDs and the other odd-numbered. (load balancer)
		- **Pre-generating the News Feed:** We can have dedicated servers that are continuously generating users’ News Feeds and storing them in a ‘UserNewsFeed’ table. So whenever any user needs the latest photos for their News-Feed, we will simply query this table and return the results to the user.
			- **Pull or Push?**
				- **Hybrid:** We can adopt a hybrid approach. We can move all the users who have a high number of followers to a pull-based model and only push data to those who have a few hundred (or thousand) follows. Another approach could be that the server pushes updates to all the users not more than a certain frequency and letting users with a lot of followers/updates to pull data regularly.
	- ### Designing Dropbox
	  collapsed:: true
		- **Availability:** The motto of cloud storage services is to have data availability anywhere, anytime. Users can access their files/photos from any device whenever and wherever they like.
		- **Reliability and Durability:** Another benefit of cloud storage is that it offers 100% reliability and durability of data. Cloud storage ensures that users will never lose their data by keeping multiple copies of the data stored on different geographically located servers.
		- **Requirements**, **Some Design Considerations**
			- We should expect huge read and write volumes. Read to write ratio is expected to be nearly the same.
			- Keeping a local copy of the metadata (file name, size, etc.) with the client can save us a lot of round trips to the server.
			- For small changes, clients can intelligently upload the diffs instead of the whole chunk.
		- **High Level Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/a754f17f56ef778b4bfb9fc9dc44b1cc.png)
		- **Component Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/be5db77680fed602be8334e2ab24f9a6.png)
			- **Client**
				- The Client Application monitors the workspace folder on the user’s machine and syncs all files/folders in it with the remote Cloud Storage.
				- **How do we handle file transfer efficiently?**
					- Chunk: divide each file into fixed sizes of 4MB chunks.
				- **Should we keep a copy of metadata with Clients?**
					- YES. Keeping a local copy of metadata not only enables us to do offline updates but also saves a lot of round trips to update remote metadata.
				- **How can clients efficiently listen to changes happening with other clients?**
					- periodically check
					- HTTP long polling
				- **divide our client into four parts**
					- **Internal Metadata Database**
					- **Chunker**
					- **Watcher**
					- **Indexer** (events handler)
			- **Metadata Database**
				- The Metadata Database is responsible for maintaining the versioning and metadata information about files/chunks, users, and workspaces.
				- SQL or NoSQL?
					- a consistent view
					- ACID!
			- **Synchronization Service**
				- The Synchronization Service is the component that processes file updates made by a client and applies these changes to other subscribed clients.
				- The Synchronization Service is the most important part of the system architecture due to its critical role in managing the metadata and synchronizing users’ files.
				- The Synchronization Service should be designed to transmit less data between clients and the Cloud Storage to achieve a better response time.
					- just transmit the difference between two versions of a file
					- dividing our files into 4MB chunks
					- calculate a hash (e.g., SHA-256) to see whether to update the local copy of a chunk or not
				- The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies.
			- **Message Queuing Service**
				- An important part of our architecture is a messaging middleware that should be able to handle a substantial number of requests. A scalable Message Queuing Service that supports asynchronous message-based communication between clients and the Synchronization Service best fits the requirements of our application.
				- ![](https://lwfiles.mycourse.app/systemdesign-public/5f5dd592905d3126b83b8a05d57fc9ae.png)
			- **Cloud/Block Storage**
				- Cloud/Block Storage stores chunks of files uploaded by the users.
		- **File Processing Workflow**
			- The sequence below shows the interaction between the components of the application in a scenario when Client A updates a file that is shared with Client B and C, so they should receive the update too. If the other clients are not online at the time of the update, the Message Queuing Service keeps the update notifications in separate response queues for them until they come online later.
			- **Procedure**
				- Client A uploads chunks to cloud storage.
				- Client A updates metadata and commits changes.
				- Client A gets confirmation and notifications are sent to Clients B and C about the changes.
				- Client B and C receive metadata changes and download updated chunks.
		- **Data Deduplication**
			- For each new incoming chunk, we can calculate a hash of it and compare that hash with all the hashes of the existing chunks to see if we already have the same chunk present in our storage.
			- **Post-process deduplication**
			- **In-line deduplication**
		- **Metadata Partitioning**
			- To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks.
		-
	- ### Designing Facebook Messenger
	  collapsed:: true
		- Messenger’s high availability is desirable; we can tolerate lower availability in the interest of consistency.
		- **Messages Handling**
			- **How would we efficiently send/receive messages?**
				- **Push model:** Users can keep a connection open with the server and can depend upon the server to notify them whenever there are new messages.
			- **How can the server keep track of all the opened connections to efficiently redirect messages to the users?**
				- The server can maintain a hash table, where “key” would be the UserID and “value” would be the connection object. So whenever the server receives a message for a user, it looks up that user in the hash table to find the connection object and sends the message on the open request.
			- **What will happen when the server receives a message for a user who has gone offline?**
				- If the receiver has disconnected, the server can notify the sender about the delivery failure. However, if it is a temporary disconnect, e.g., the receiver’s long-poll request just timed out, then we should expect a reconnect from the user. In that case, we can ask the sender to retry sending the message. This retry could be embedded in the client’s logic so that users don’t have to retype the message. The server can also store the message for a while and retry sending it once the receiver reconnects.
			- **How many chat servers do we need?**
				- Let’s plan for 500 million connections at any time. Assuming **a modern server can handle 50K concurrent connections** at any time, we would need 10K such servers.
			- **How should the server process a ‘deliver message’ request?**
				- 1) Store the message in the database
				- 2) Send the message to the receiver
				- 3) Send an acknowledgment to the sender.
			- **How does the messenger maintain the sequencing of the messages?**
				- We need to keep a sequence number with every message for each client. This sequence number will determine the exact ordering of messages for EACH user.
		- **Storing and retrieving the messages from the database**
			- We have to keep certain things in mind while designing our database:
				- How to efficiently work with the database connection pool.
				- How to retry failed requests.
				- Where to log those requests that failed even after some retries.
				- How to retry these logged requests (that failed after the retry) when all the issues have been resolved.
			- **Which storage system should we use?**
				- We need to have a database that can support a very high rate of small updates and also fetch a range of records quickly.
				- We cannot use RDBMS like MySQL or NoSQL like MongoDB because we cannot afford to read/write a row from the database every time a user receives/sends a message. This will not only make the basic operations of our service run with high latency but also create a huge load on databases.
				- Both of our requirements can be easily met with a wide-column database solution like .
		- **Cache**
			- We can cache a few recent messages (say last 15) in a few recent conversations that are visible in a user’s viewport (say last 5). Since we decided to store all of the user’s messages on one shard, the cache for a user should entirely reside on one machine too.
		- **Fault tolerance and Replication**
			- **Should we store multiple copies of user messages?** We cannot store only one copy of the user’s data because if the server holding the data crashes or is down permanently, we don’t have any mechanism to recover that data. For this, either we have to store multiple copies of the data on different servers or use techniques like [Reed-Solomon encoding](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction) to distribute and replicate it.
		-
			-
			-
			-
	- ### Designing Twitter
	  collapsed:: true
		- **Requirements**
			- The service should be able to create and display a user’s timeline consisting of top tweets from all the people the user follows.
			- Our service needs to be highly available.
			- Acceptable latency of the system is 200ms for timeline generation.
			- Consistency can take a hit (in the interest of availability); if a user doesn’t see a tweet for a while, it should be fine.
		- **Data Sharding**
			- **Sharding based on UserID**
				- What if a user becomes hot? There could be a lot of queries on the server holding the user. This high load will affect the performance of our service.
				- Over time some users can end up storing a lot of tweets or having a lot of follows compared to others. Maintaining a uniform distribution of growing user data is quite difficult.
			- **Sharding based on TweetID**
				- We have to query all database partitions to find tweets of a user, which can result in higher latencies.
			- **Sharding based on Tweet creation time**
				- While writing, all new tweets will be going to one server and the remaining servers will be sitting idle.
				- Similarly, while reading, the server holding the latest data will have a very high load as compared to servers holding old data.
			- **combine sharding by TweetID and Tweet creation time**
				- our TweetID will have two parts: the first part will be representing epoch seconds and the second part will be an auto-incrementing sequence. So, to make a new TweetID, we can take the current epoch time and append an auto-incrementing number to it.
		- **Monitoring**
			- By monitoring these counters, we will realize if we need more replication, load balancing, or caching.
	- ### Designing Youtube or Netflix
	  collapsed:: true
		- **Requirements**
			- The system should be highly reliable, any video uploaded should not be lost.
			- Consistency can take a hit (in the interest of availability); if a user doesn’t see a video for a while, it should be fine.
			- Users should have a real-time experience while watching videos and should not feel any lag.
		- **System APIs**
			- uploadVideo
			- searchVideo: page_token
				- page_token (string): This token will specify a page in the result set that should be returned.
		- **High Level Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/00dcb64c2c3e55355afce8bc1447fffe.png)
			- **Thumbnails generator:** To generate a few thumbnails for each video.
		- **Detailed Component Design**
			- **Where would videos be stored?** Videos can be stored in a distributed file storage system like HDFS or [GlusterFS](https://en.wikipedia.org/wiki/Gluster#GlusterFS).
			- **How should we efficiently manage read traffic?**
				- We should segregate our read traffic from write traffic.
			- **Where would thumbnails be stored?**
				- There will be a lot more thumbnails than videos.
				- If we assume that every video will have five thumbnails, we need to have a very efficient storage system that can serve huge read traffic.
				- Thumbnails are small files, say, a maximum of 5KB each.
				- Read traffic for thumbnails will be huge compared to videos. Users will be watching one video at a time, but they might be looking at a page with 20 thumbnails of other videos.
				- [Bigtable](https://en.wikipedia.org/wiki/Bigtable) can be a reasonable choice here as it combines multiple files into one block to store on the disk and is very efficient in reading a small amount of data. Both of these are the two most significant requirements for our service.
			- **Video Uploads:** Since videos could be huge, if while uploading, the connection drops, we should support resuming from the same point.
		- **Video Deduplication**
			- Duplicate videos often differ in aspect ratios or encodings, contain overlays or additional borders, or be excerpts from a longer original video.
			- For our service, deduplication makes most sense early; when a user is uploading a video as compared to post-processing it to find duplicate videos later. Inline deduplication will save us a lot of resources that can be used to encode, transfer, and store the duplicate copy of the video.
			- As soon as any user starts uploading a video, our service can run video matching algorithms (e.g., [Block Matching](https://en.wikipedia.org/wiki/Block-matching_algorithm), [Phase correlation](https://en.wikipedia.org/wiki/Phase_correlation), etc.) to find duplications.
		- **Load Balancing**
			- Since we will be using a static hash-based scheme to map videos to hostnames, it can lead to an uneven load on the logical replicas due to each video’s different popularity.
			- For instance, if a video becomes popular, the logical replica corresponding to that video will experience more traffic than other servers. These uneven loads for logical replicas can then translate into uneven load distribution on corresponding physical servers.
			- To resolve this issue, any busy server in one location can redirect a client to a less busy server in the same cache location. We can use dynamic HTTP redirections for this scenario.
			- However, the use of redirections also has its drawbacks.
				- First, since our service tries to load balance locally, it leads to multiple redirections if the host that receives the redirection can't serve the video.
				- Also, each redirection requires a client to make an additional HTTP request; it also leads to higher delays before the video starts playing back.
				- Moreover, inter-tier (or cross data-center) redirections lead a client to a distant cache location because the higher tier caches are only present at a small number of locations.
			- ##
	- ### Designing Typeahead Suggestion
		- **Requirements**
			- **Non-function**: The suggestions should appear in real-time. The user should be able to see the suggestions within 200ms.
		- **Basic System Design and Algorithm**
			- As we have to serve a lot of queries with minimum latency, we need to come up with a scheme that can efficiently store our data such that it can be queried quickly.
			- We can't depend upon some database for this; we need to store our index in memory in a highly efficient data structure.
			- One of the most appropriate data structures that can serve our purpose is the Trie.
		- **Permanent Storage**
			-
			-
- ## Glossary of System Design Basics
	- ### System Design Basics
	  collapsed:: true
		- Whenever we are designing a large system, we need to consider a few things:
			- What are the different architectural pieces that can be used?
			- How do these pieces work with each other?
			- How can we best utilize these pieces: what are the right tradeoffs?
		- Investing in scaling before it is needed is generally not a smart business proposition
	- ### Key Characteristics of Distributed Systems
		- Key characteristics of a distributed system include **Scalability**, **Reliability**, **Availability**, **Efficiency**, and **Manageability**.
		- **Scalability**
			- Scalability is the capability of a system, process, or a network to grow and manage increased demand.
			- **Horizontal vs. Vertical Scaling:** Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server.
		- **Reliability**
			- By definition, reliability is the probability a system will fail in a given period. In simple terms, a distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail.
		- **Availability**
			- By definition, availability is the time a system remains operational to perform its required function in a specific period.
			- Reliability is availability over time considering the full range of possible real-world conditions that can occur.
		- **Efficiency**
			- wo standard measures of its efficiency are the response time (or latency) that denotes the delay to obtain the first item and the throughput (or bandwidth) which denotes the number of items delivered in a given time unit (e.g., a second).
		- **Serviceability** or **Manageability**
			- Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained; if the time to fix a failed system increases, then availability will decrease. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate (i.e., does it routinely operate without failure or exceptions?).
			- Early detection of faults can decrease or avoid system downtime. For example, some enterprise systems can automatically call a service center (without human intervention) when the system experiences a system fault.
			-
	- ### Caching
	  collapsed:: true
		- **Application server cache**
			- Placing a cache directly on a request layer node enables the local storage of response data.
		- **Content Delivery (or Distribution) Network (CDN)**
		- **Cache Invalidation**
			- **Write-through cache:** Under this scheme, data is written into the cache and the corresponding database simultaneously.
				- minimizes the risk of data loss
				- higher latency for write operations
			- **Write-around cache:** This technique is similar to write-through cache, but data is written directly to permanent storage, bypassing the cache.
				- a read request for recently written data will create a “cache miss” and must be read from slower back-end storage and experience higher latency.
			- **Write-back cache:** Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client.
				- low-latency and high-throughput for write-intensive applications
				- risk of data loss
		- **Cache eviction policies**
			- First In First Out (FIFO)
			- Last In First Out (LIFO)
			- Least Recently Used (LRU)
			- Most Recently Used (MRU):
			- Least Frequently Used (LFU)
			- Random Replacement (RR)
	- ### SQL vs. NoSQL
	  collapsed:: true
		- Relational databases are structured and have predefined schemas like phone books that store phone numbers and addresses.
		- Non-relational databases are unstructured, distributed, and have a dynamic schema like file folders that hold everything from a person’s address and phone number to their Facebook ‘likes’ and online shopping preferences.
		- **SQL**
			- Relational databases store data in rows and columns. Each row contains all the information about one entity and each column contains all the separate data points.
			- Some of the most popular relational databases are MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB.
		- **NoSQL**
			- **Key-Value Stores**: Redis, Voldemort, and Dynamo
			- **Document Databases**: CouchDB and MongoDB
			- **Wide-Column Databases**
				- In columnar databases we have column families, which are containers for rows.
				- Unlike relational databases, we don’t need to know all the columns up front and each row doesn’t have to have the same number of columns.
				- Columnar databases are best suited for analyzing large datasets - big names include Cassandra and HBase.
			- **Graph Databases**
				- Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities).
				- Examples of graph database include Neo4J and InfiniteGraph.
		- **High level differences between SQL and NoSQL**
			- **Storage**
			- **Schema**
			- **Querying**
			- **Scalability**
				- In most common situations, SQL databases are vertically scalable, NoSQL databases are horizontally scalable.
			- **Reliability or ACID Compliancy (Atomicity, Consistency, Isolation, Durability)**
				- The vast majority of relational databases are ACID compliant.
				- Most of the NoSQL solutions sacrifice ACID compliance for performance and scalability.
		- **Which one to use?**
			- When it comes to database technology, there’s no one-size-fits-all solution.
			- Many businesses rely on both relational and non-relational databases for different needs
			- **Reasons to use SQL database**
				- We need to ensure ACID compliance.
				- Your data is structured and unchanging.
			- **Reasons to use NoSQL database**
				- Storing large volumes of data that often have little to no structure.
				- Making the most of cloud computing and storage.
				- Rapid development.
	- ### Consistent Hashing
	  collapsed:: true
		- David Karger et al. first introduced Consistent Hashing in their [1997 paper](https://dl.acm.org/doi/10.1145/258533.258660) and suggested its use in distributed caching.
		- The act of distributing data across a set of nodes is called data partitioning.
			- How do we know on which node a particular piece of data will be stored?
			- When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Additionally, how can we minimize data movement when nodes join or leave?
		- Consistent Hashing maps data to physical nodes and ensures that **only a small set of keys move when servers are added or removed.**
		- Vnodes are **randomly distributed** across the cluster and are generally **non-contiguous** so that no two neighboring Vnodes are assigned to the same physical node or rack.
		- **Dynamo** and **Cassandra** use Consistent Hashing to distribute their data across nodes.
	- ### Long-Polling vs WebSockets vs Server-Sent Events
	  collapsed:: true
		- **HTTP Long-Polling**
			- With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately. That’s why this technique is sometimes referred to as a “Hanging GET”.
		- **WebSockets**
			- WebSocket provides [Full duplex](https://en.wikipedia.org/wiki/Duplex_(telecommunications)#Full_duplex) communication channels over a single TCP connection.
		- **Server-Sent Events (SSEs)**
			- Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. If the client wants to send data to the server, it would require the use of another technology/protocol to do so.
	- ### Bloom Filters
		- The Bloom filter data structure tells whether an element **may be in a set, or definitely is not**.
		-
		-