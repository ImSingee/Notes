- [课程链接](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview)
-
- ## System Design Guide
  collapsed:: true
	- ### System Design Interviews: A step by step guide
	  collapsed:: true
		- **Step 1: Requirements clarifications**
			- It is always a good idea to ask questions about the exact scope of the problem we are trying to solve. Candidates who spend enough time to define the end goals of the system always have a better chance to be successful in the interview.
		- **Step 2: Back-of-the-envelope estimation**
			- It is always a good idea to estimate the scale of the system we’re going to design. This will also help later when we focus on scaling, partitioning, load balancing, and caching.
			- scale, storage, network bandwidth
		- **Step 3: System interface definition**
			- Define what APIs are expected from the system. This will establish the exact contract expected from the system and ensure if we haven’t gotten any requirements wrong.
		- **Step 4: Defining data model**
			- Defining the data model in the early part of the interview will clarify how data will flow between different system components.
			- Later, it will guide for data partitioning and management.
				- storage, transportation, encryption
		- **Step 5: High-level design**
			- Draw a block diagram with **5-6 boxes** representing the core components of our system. We should identify enough components that are needed to solve the actual problem from end to end.
		- **Step 6: Detailed design**
			- Dig deeper into two or three major components; the interviewer’s feedback should always guide us to what parts of the system need further discussion.
			- We should present different approaches, their pros and cons, and explain why we will prefer one approach over the other.
				- Remember, there is no single answer; the only important thing is to consider tradeoffs between different options while keeping system constraints in mind.
		- **Step 7: Identifying and resolving bottlenecks**
			- Try to discuss as many bottlenecks as possible and different approaches to mitigate them.
				- single point of failure
				- replicas of the data
				- enough copies of different services
				- monitoring the performance, get alerts whenever critical components fail or their performance degrades
		- Preparation and being organized during the interview are the keys to success in system design interviews.
	- ### Designing a URL Shortening service like TinyURL
	  collapsed:: true
		- **1. Why do we need URL shortening?**
		- **2. Requirements and Goals of the System**
			- You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind.
			- Functional Requirements, Non-Functional Requirements, Extended Requirements
		- **3. Capacity Estimation and Constraints**
			- **Traffic estimates**, **Storage estimates**, **Bandwidth estimates**, **Memory estimates**
			- **High-level estimates** (Summary)
		- **4. System APIs**
			- Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system.
			- **How do we detect and prevent abuse?**
		- **5. Database Design**
			- Defining the DB schema in the early stages of the interview would help to understand the data flow among various components and later would guide towards data partitioning.
			- **Database Schema**
			- **What kind of database should we use?**
				- [SQL vs. NoSQL](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview&unit=grokking-the-system-design-interview_1627054379423_8Unit)
		- **6. Basic System Design and Algorithm**
			- Hash or **Key Generation Service (KGS)**
		- **7. Data Partitioning and Replication**
			- **Range Based Partitioning** or **Hash-Based Partitioning**
				- [Consistent Hashing](https://designgurus.org/path-player?courseid=grokking-the-system-design-interview&unit=grokking-the-system-design-interview_1627054411532_11Unit)
		- **8. Cache**
			- **cache memory**, **cache eviction policy **
		- **9. Load Balancer (LB)**
		- **10. Purging or DB cleanup**
		- **11. Telemetry**
		- **12. Security and Permissions**
			- permission level (public/private)
- ## System Design Problems
	- ### Designing Pastebin
	  collapsed:: true
		- We can assume a 5:1 ratio between the read and write.
		- Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs, or logs. Such texts are not huge, so let’s assume that each paste on average contains 10KB.
		- To keep some margin, we will assume a 70% capacity model (meaning we don’t want to use more than 70% of our total storage capacity at any point),
		- Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes.
		- ‘ContentKey’ is a reference to an external object storing the contents of the paste
		- Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don’t see failure due to the duplicate key.
	- ### Designing Instagram
	  collapsed:: true
		- High Availability
		- The acceptable latency of the system is 200ms for News Feed generation.
		- We may need to use SQL database, but relational databases come with their challenges, especially when we need to scale them.
		- In data stores, deletes don’t get applied instantly; data is retained for certain days (to support undeleting) before getting removed from the system permanently.
		- We can split reads and writes into separate services. We will have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system.
		- Losing files is not an option for our service. Therefore, we will store multiple copies of each file so that if one storage server dies, we can retrieve the photo from the other copy present on a different storage server.
		- KGS single point of failure: A workaround for that could be to define two such databases, one generating even-numbered IDs and the other odd-numbered. (load balancer)
		- **Pre-generating the News Feed:** We can have dedicated servers that are continuously generating users’ News Feeds and storing them in a ‘UserNewsFeed’ table. So whenever any user needs the latest photos for their News-Feed, we will simply query this table and return the results to the user.
			- **Pull or Push?**
				- **Hybrid:** We can adopt a hybrid approach. We can move all the users who have a high number of followers to a pull-based model and only push data to those who have a few hundred (or thousand) follows. Another approach could be that the server pushes updates to all the users not more than a certain frequency and letting users with a lot of followers/updates to pull data regularly.
	- ### Designing Dropbox
	  collapsed:: true
		- **Availability:** The motto of cloud storage services is to have data availability anywhere, anytime. Users can access their files/photos from any device whenever and wherever they like.
		- **Reliability and Durability:** Another benefit of cloud storage is that it offers 100% reliability and durability of data. Cloud storage ensures that users will never lose their data by keeping multiple copies of the data stored on different geographically located servers.
		- **Requirements**, **Some Design Considerations**
			- We should expect huge read and write volumes. Read to write ratio is expected to be nearly the same.
			- Keeping a local copy of the metadata (file name, size, etc.) with the client can save us a lot of round trips to the server.
			- For small changes, clients can intelligently upload the diffs instead of the whole chunk.
		- **High Level Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/a754f17f56ef778b4bfb9fc9dc44b1cc.png)
		- **Component Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/be5db77680fed602be8334e2ab24f9a6.png)
			- **Client**
				- The Client Application monitors the workspace folder on the user’s machine and syncs all files/folders in it with the remote Cloud Storage.
				- **How do we handle file transfer efficiently?**
					- Chunk: divide each file into fixed sizes of 4MB chunks.
				- **Should we keep a copy of metadata with Clients?**
					- YES. Keeping a local copy of metadata not only enables us to do offline updates but also saves a lot of round trips to update remote metadata.
				- **How can clients efficiently listen to changes happening with other clients?**
					- periodically check
					- HTTP long polling
				- **divide our client into four parts**
					- **Internal Metadata Database**
					- **Chunker**
					- **Watcher**
					- **Indexer** (events handler)
			- **Metadata Database**
				- The Metadata Database is responsible for maintaining the versioning and metadata information about files/chunks, users, and workspaces.
				- SQL or NoSQL?
					- a consistent view
					- ACID!
			- **Synchronization Service**
				- The Synchronization Service is the component that processes file updates made by a client and applies these changes to other subscribed clients.
				- The Synchronization Service is the most important part of the system architecture due to its critical role in managing the metadata and synchronizing users’ files.
				- The Synchronization Service should be designed to transmit less data between clients and the Cloud Storage to achieve a better response time.
					- just transmit the difference between two versions of a file
					- dividing our files into 4MB chunks
					- calculate a hash (e.g., SHA-256) to see whether to update the local copy of a chunk or not
				- The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies.
			- **Message Queuing Service**
				- An important part of our architecture is a messaging middleware that should be able to handle a substantial number of requests. A scalable Message Queuing Service that supports asynchronous message-based communication between clients and the Synchronization Service best fits the requirements of our application.
				- ![](https://lwfiles.mycourse.app/systemdesign-public/5f5dd592905d3126b83b8a05d57fc9ae.png)
			- **Cloud/Block Storage**
				- Cloud/Block Storage stores chunks of files uploaded by the users.
		- **File Processing Workflow**
			- The sequence below shows the interaction between the components of the application in a scenario when Client A updates a file that is shared with Client B and C, so they should receive the update too. If the other clients are not online at the time of the update, the Message Queuing Service keeps the update notifications in separate response queues for them until they come online later.
			- **Procedure**
				- Client A uploads chunks to cloud storage.
				- Client A updates metadata and commits changes.
				- Client A gets confirmation and notifications are sent to Clients B and C about the changes.
				- Client B and C receive metadata changes and download updated chunks.
		- **Data Deduplication**
			- For each new incoming chunk, we can calculate a hash of it and compare that hash with all the hashes of the existing chunks to see if we already have the same chunk present in our storage.
			- **Post-process deduplication**
			- **In-line deduplication**
		- **Metadata Partitioning**
			- To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks.
		-
	- ### Designing Facebook Messenger
	  collapsed:: true
		- Messenger’s high availability is desirable; we can tolerate lower availability in the interest of consistency.
		- **Messages Handling**
			- **How would we efficiently send/receive messages?**
				- **Push model:** Users can keep a connection open with the server and can depend upon the server to notify them whenever there are new messages.
			- **How can the server keep track of all the opened connections to efficiently redirect messages to the users?**
				- The server can maintain a hash table, where “key” would be the UserID and “value” would be the connection object. So whenever the server receives a message for a user, it looks up that user in the hash table to find the connection object and sends the message on the open request.
			- **What will happen when the server receives a message for a user who has gone offline?**
				- If the receiver has disconnected, the server can notify the sender about the delivery failure. However, if it is a temporary disconnect, e.g., the receiver’s long-poll request just timed out, then we should expect a reconnect from the user. In that case, we can ask the sender to retry sending the message. This retry could be embedded in the client’s logic so that users don’t have to retype the message. The server can also store the message for a while and retry sending it once the receiver reconnects.
			- **How many chat servers do we need?**
				- Let’s plan for 500 million connections at any time. Assuming **a modern server can handle 50K concurrent connections** at any time, we would need 10K such servers.
			- **How should the server process a ‘deliver message’ request?**
				- 1) Store the message in the database
				- 2) Send the message to the receiver
				- 3) Send an acknowledgment to the sender.
			- **How does the messenger maintain the sequencing of the messages?**
				- We need to keep a sequence number with every message for each client. This sequence number will determine the exact ordering of messages for EACH user.
		- **Storing and retrieving the messages from the database**
			- We have to keep certain things in mind while designing our database:
				- How to efficiently work with the database connection pool.
				- How to retry failed requests.
				- Where to log those requests that failed even after some retries.
				- How to retry these logged requests (that failed after the retry) when all the issues have been resolved.
			- **Which storage system should we use?**
				- We need to have a database that can support a very high rate of small updates and also fetch a range of records quickly.
				- We cannot use RDBMS like MySQL or NoSQL like MongoDB because we cannot afford to read/write a row from the database every time a user receives/sends a message. This will not only make the basic operations of our service run with high latency but also create a huge load on databases.
				- Both of our requirements can be easily met with a wide-column database solution like .
		- **Cache**
			- We can cache a few recent messages (say last 15) in a few recent conversations that are visible in a user’s viewport (say last 5). Since we decided to store all of the user’s messages on one shard, the cache for a user should entirely reside on one machine too.
		- **Fault tolerance and Replication**
			- **Should we store multiple copies of user messages?** We cannot store only one copy of the user’s data because if the server holding the data crashes or is down permanently, we don’t have any mechanism to recover that data. For this, either we have to store multiple copies of the data on different servers or use techniques like [Reed-Solomon encoding](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction) to distribute and replicate it.
		-
			-
			-
			-
	- ### Designing Twitter
	  collapsed:: true
		- **Requirements**
			- The service should be able to create and display a user’s timeline consisting of top tweets from all the people the user follows.
			- Our service needs to be highly available.
			- Acceptable latency of the system is 200ms for timeline generation.
			- Consistency can take a hit (in the interest of availability); if a user doesn’t see a tweet for a while, it should be fine.
		- **Data Sharding**
			- **Sharding based on UserID**
				- What if a user becomes hot? There could be a lot of queries on the server holding the user. This high load will affect the performance of our service.
				- Over time some users can end up storing a lot of tweets or having a lot of follows compared to others. Maintaining a uniform distribution of growing user data is quite difficult.
			- **Sharding based on TweetID**
				- We have to query all database partitions to find tweets of a user, which can result in higher latencies.
			- **Sharding based on Tweet creation time**
				- While writing, all new tweets will be going to one server and the remaining servers will be sitting idle.
				- Similarly, while reading, the server holding the latest data will have a very high load as compared to servers holding old data.
			- **combine sharding by TweetID and Tweet creation time**
				- our TweetID will have two parts: the first part will be representing epoch seconds and the second part will be an auto-incrementing sequence. So, to make a new TweetID, we can take the current epoch time and append an auto-incrementing number to it.
		- **Monitoring**
			- By monitoring these counters, we will realize if we need more replication, load balancing, or caching.
	- ### Designing Youtube or Netflix
	  collapsed:: true
		- **Requirements**
			- The system should be highly reliable, any video uploaded should not be lost.
			- Consistency can take a hit (in the interest of availability); if a user doesn’t see a video for a while, it should be fine.
			- Users should have a real-time experience while watching videos and should not feel any lag.
		- **System APIs**
			- uploadVideo
			- searchVideo: page_token
				- page_token (string): This token will specify a page in the result set that should be returned.
		- **High Level Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/00dcb64c2c3e55355afce8bc1447fffe.png)
			- **Thumbnails generator:** To generate a few thumbnails for each video.
		- **Detailed Component Design**
			- **Where would videos be stored?** Videos can be stored in a distributed file storage system like HDFS or [GlusterFS](https://en.wikipedia.org/wiki/Gluster#GlusterFS).
			- **How should we efficiently manage read traffic?**
				- We should segregate our read traffic from write traffic.
			- **Where would thumbnails be stored?**
				- There will be a lot more thumbnails than videos.
				- If we assume that every video will have five thumbnails, we need to have a very efficient storage system that can serve huge read traffic.
				- Thumbnails are small files, say, a maximum of 5KB each.
				- Read traffic for thumbnails will be huge compared to videos. Users will be watching one video at a time, but they might be looking at a page with 20 thumbnails of other videos.
				- [Bigtable](https://en.wikipedia.org/wiki/Bigtable) can be a reasonable choice here as it combines multiple files into one block to store on the disk and is very efficient in reading a small amount of data. Both of these are the two most significant requirements for our service.
			- **Video Uploads:** Since videos could be huge, if while uploading, the connection drops, we should support resuming from the same point.
		- **Video Deduplication**
			- Duplicate videos often differ in aspect ratios or encodings, contain overlays or additional borders, or be excerpts from a longer original video.
			- For our service, deduplication makes most sense early; when a user is uploading a video as compared to post-processing it to find duplicate videos later. Inline deduplication will save us a lot of resources that can be used to encode, transfer, and store the duplicate copy of the video.
			- As soon as any user starts uploading a video, our service can run video matching algorithms (e.g., [Block Matching](https://en.wikipedia.org/wiki/Block-matching_algorithm), [Phase correlation](https://en.wikipedia.org/wiki/Phase_correlation), etc.) to find duplications.
		- **Load Balancing**
			- Since we will be using a static hash-based scheme to map videos to hostnames, it can lead to an uneven load on the logical replicas due to each video’s different popularity.
			- For instance, if a video becomes popular, the logical replica corresponding to that video will experience more traffic than other servers. These uneven loads for logical replicas can then translate into uneven load distribution on corresponding physical servers.
			- To resolve this issue, any busy server in one location can redirect a client to a less busy server in the same cache location. We can use dynamic HTTP redirections for this scenario.
			- However, the use of redirections also has its drawbacks.
				- First, since our service tries to load balance locally, it leads to multiple redirections if the host that receives the redirection can't serve the video.
				- Also, each redirection requires a client to make an additional HTTP request; it also leads to higher delays before the video starts playing back.
				- Moreover, inter-tier (or cross data-center) redirections lead a client to a distant cache location because the higher tier caches are only present at a small number of locations.
			- ##
	- ### Designing Typeahead Suggestion
	  collapsed:: true
		- **Requirements**
			- **Non-function**: The suggestions should appear in real-time. The user should be able to see the suggestions within 200ms.
		- **Basic System Design and Algorithm**
			- As we have to serve a lot of queries with minimum latency, we need to come up with a scheme that can efficiently store our data such that it can be queried quickly.
			- We can't depend upon some database for this; we need to store our index in memory in a highly efficient data structure.
			- One of the most appropriate data structures that can serve our purpose is the Trie.
		- **Permanent Storage**
		- **Data Partition**
			- **Range Based Partitioning**: based on their first letter
				- The main problem with this approach is that it can lead to unbalanced servers.
			- **Partition based on the maximum capacity of the server**
				- We can keep storing data on a server as long as it has memory available.
				- We can keep a hash table to quickly access this partitioning scheme
				- We can have a load balancer in front of our trie servers which can store this mapping and redirect traffic. Also, if we are querying from multiple servers, either we need to merge the results on the server-side to calculate the overall top results or make our clients do that.
					- If we prefer to do this on the server-side, we need to introduce another layer of servers between load balancers and trie severs (let’s call them aggregator). These servers will aggregate results from multiple trie servers and return the top results to the client.
				- Partitioning based on the maximum capacity can still lead us to hotspots
			- **Partition based on the hash of the term**
				- The disadvantage of this scheme is, to find typeahead suggestions for a term we have to ask all the servers and then aggregate the results.
		- **Cache**
			- We can have separate cache servers in front of the trie servers holding the most frequently searched terms and their typeahead suggestions.
		- **Typeahead Client**
			- The client should only try hitting the server if the user has not pressed any key for 50ms. (debounce)
			- If the user is constantly typing, the client can cancel the in-progress requests.
			- Initially, the client can wait until the user enters a couple of characters.
			- Clients can pre-fetch some data from the server to save future requests.
			- Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused.
			- Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn’t waste time in establishing the connection.
			- The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.
	- ### Designing an API Rate Limiter
	  collapsed:: true
		- **What is**
			- Imagine we have a service which is receiving a huge number of requests, but it can only serve a limited number of requests per second.
			- To handle this problem we would need some kind of throttling or rate limiting mechanism that would allow only a certain number of requests so our service can respond to all of them.
			- A rate limiter, at a high-level, limits the number of events an entity (user, device, IP, etc.) can perform in a particular time window.
		- **Why do we need**
			- to protect services against abusive behaviors
				- attacks
				- brute-force password attempts
				- brute-force credit card transactions
			- to make a service (or API) more reliable
				- Misbehaving clients/scripts
				- Security
				- To prevent abusive behavior and bad design practices
				- To keep costs and resource usage under control
				- Revenue
				- To eliminate spikiness in traffic
		- **How to**
			- **Rate Limiting** is a process that is used to define the rate and speed at which consumers can access APIs.
			- **Throttling** is the process of controlling the usage of the APIs by customers during a given period.
				- Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the server returns HTTP status “429 - Too many requests".
		- **What are different types of throttling?**
			- **Hard Throttling:** The number of API requests cannot exceed the throttle limit.
			- **Soft Throttling:** In this type, we can set the API request limit to exceed a certain percentage. For example, if we have rate-limit of 100 messages a minute and 10% exceed-limit, our rate limiter will allow up to 110 messages per minute.
			- **Elastic or Dynamic Throttling**: Under Elastic throttling, the number of requests can go beyond the threshold if the system has some resources available. For example, if a user is allowed only 100 messages a minute, we can let the user send more than 100 messages a minute when there are free resources available in the system.
		- **What are different types of algorithms used for Rate Limiting?**
			- **Fixed Window Algorithm:** In this algorithm, the time window is considered from the start of the time-unit to the end of the time-unit.
			- **Rolling Window Algorithm:** In this algorithm, the time window is considered from the fraction of the time at which the request is made plus the time window length.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/dc173b9d8e6c8b4b24a4293355ca3107.png?client_id=60da7e92ee99a6342460f6f2&width=722&height=217)
		- **High level design for Rate Limiter**
			- Once a new request arrives, the Web Server first asks the Rate Limiter to decide if it will be served or throttled. If the request is not throttled, then it’ll be passed to the API servers.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/8dda5b2899a98b62ec83a46589b821cb.png)
		- **Algorithm**
		- **Data Sharding and Caching**
			- We can shard based on the ‘UserID’ to distribute the user’s data.
				- If we want to have different throttling limits for different APIs, we can choose to shard per user per API.
			- Our system can get huge benefits from caching recent active users.
			- Our rate limiter can significantly benefit from the **Write-back cache** by updating all counters and timestamps in cache only. The write to the permanent storage can be done at fixed intervals.
			-
	- ### Designing Twitter Search
	  collapsed:: true
		- ![](https://lwfiles.mycourse.app/systemdesign-public/49466721dfb137a5bd56711167217c2e.png?client_id=60da7e92ee99a6342460f6f2&width=600&height=280)
		-
	- ### Designing Facebook's Newsfeed
		- **Requirements**
			- Our system should be able to generate any user's newsfeed in real-time - maximum latency seen by the end user would be 2s.
			- A post shouldn't take more than 5s to make it to a user's feed assuming a new newsfeed request comes in.
		- **Database Design**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/6177f836fcf3af7d92c7935efdbd0736.png)
		- **High Level System Design**
			- **Feed generation**
				- Newsfeed is generated from the posts (or feed items) of users and entities (pages and groups) that a user follows.
			- **Feed publishing**
				-
			-
	-
- ## Glossary of System Design Basics
	- ### System Design Basics
	  collapsed:: true
		- Whenever we are designing a large system, we need to consider a few things:
			- What are the different architectural pieces that can be used?
			- How do these pieces work with each other?
			- How can we best utilize these pieces: what are the right tradeoffs?
		- Investing in scaling before it is needed is generally not a smart business proposition
	- ### Key Characteristics of Distributed Systems
	  collapsed:: true
		- Key characteristics of a distributed system include **Scalability**, **Reliability**, **Availability**, **Efficiency**, and **Manageability**.
		- **Scalability**
			- Scalability is the capability of a system, process, or a network to grow and manage increased demand.
			- **Horizontal vs. Vertical Scaling:** Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server.
		- **Reliability**
			- By definition, reliability is the probability a system will fail in a given period. In simple terms, a distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail.
		- **Availability**
			- By definition, availability is the time a system remains operational to perform its required function in a specific period.
			- Reliability is availability over time considering the full range of possible real-world conditions that can occur.
		- **Efficiency**
			- wo standard measures of its efficiency are the response time (or latency) that denotes the delay to obtain the first item and the throughput (or bandwidth) which denotes the number of items delivered in a given time unit (e.g., a second).
		- **Serviceability** or **Manageability**
			- Another important consideration while designing a distributed system is how easy it is to operate and maintain. Serviceability or manageability is the simplicity and speed with which a system can be repaired or maintained; if the time to fix a failed system increases, then availability will decrease. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate (i.e., does it routinely operate without failure or exceptions?).
			- Early detection of faults can decrease or avoid system downtime. For example, some enterprise systems can automatically call a service center (without human intervention) when the system experiences a system fault.
			-
	- ### Load Balancing
	  collapsed:: true
		- To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add LBs at three places:
			- Between the user and the web server
			- Between web servers and an internal platform layer, like application servers or cache servers
			- Between internal platform layer and database.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/d6012052bfa02126e35a7b04df1cd800.png?client_id=60da7e92ee99a6342460f6f2&width=677&height=330)
			- ![](https://lwfiles.mycourse.app/systemdesign-public/8b99bdb0eb3359a4bb16173655c6b70b.png)
		- **Benefits of Load Balancing**
			- Users experience faster, uninterrupted service. Users won’t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource.
			- Service providers experience less downtime and higher throughput. Even a full server failure won’t affect the end user experience as the load balancer will simply route around it to a healthy server.
			- Load balancing makes it easier for system administrators to handle incoming requests while decreasing wait time for users.
			- Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the smart load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions.
			- System administrators experience fewer failed or stressed components. Instead of a single device performing a lot of work, load balancing has several devices perform a little bit of work.
		- **Load Balancing Algorithms** - **How does the load balancer choose the backend server?**
			- **Health Checks**
			- **load balancing algorithms**
				- **Least Connection Method** — This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers.
				- **Least Response Time Method** — This algorithm directs traffic to the server with the fewest active connections and the lowest average response time.
				- **Least Bandwidth Method** - This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps).
				- **Round Robin Method** — This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections.
				- **Weighted Round Robin Method** — The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights.
				- **IP Hash** — Under this method, a hash of the IP address of the client is calculated to redirect the request to a server.
		- **Redundant Load Balancers**
			- The load balancer can be a single point of failure.
			- To overcome this, a second load balancer can be connected to the first to form a cluster. Each LB monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/fddcea731c85a7007c824db3b8b0cbb9.png)
			-
	- ### Caching
	  collapsed:: true
		- **Application server cache**
			- Placing a cache directly on a request layer node enables the local storage of response data.
		- **Content Delivery (or Distribution) Network (CDN)**
		- **Cache Invalidation**
			- **Write-through cache:** Under this scheme, data is written into the cache and the corresponding database simultaneously.
				- minimizes the risk of data loss
				- higher latency for write operations
			- **Write-around cache:** This technique is similar to write-through cache, but data is written directly to permanent storage, bypassing the cache.
				- a read request for recently written data will create a “cache miss” and must be read from slower back-end storage and experience higher latency.
			- **Write-back cache:** Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client.
				- low-latency and high-throughput for write-intensive applications
				- risk of data loss
		- **Cache eviction policies**
			- First In First Out (FIFO)
			- Last In First Out (LIFO)
			- Least Recently Used (LRU)
			- Most Recently Used (MRU):
			- Least Frequently Used (LFU)
			- Random Replacement (RR)
	- ### Indexes
		- The goal of creating an index on a particular table in a database is to make it faster to search through the table and find the row or rows that we want.
		- Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.
		- **How do Indexes decrease write performance?**
			- An index can dramatically speed up data retrieval but may itself be large due to the additional keys, which slow down data insertion & update.
			- When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance.
				- performance degradation applies to all insert, update, and delete operations for the table.
	- ### Proxies
		- A proxy server is an intermediate piece of software or hardware that sits between the client and the server.
		- **Forward Proxy**
			- Typically, forward proxies are used to cache data, filter requests, log requests, or transform requests (by adding/removing headers, encrypting/decrypting, or compressing a resource).
				- A forward proxy can hide the identity of the client from the server by sending requests on behalf of the client.
			- Proxies can combine the same data access requests into one request and then return the result to the user; this technique is called **collapsed forwarding**.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/826d78641d58c32682df280dba803a8f.png)
		- **Reverse Proxy**
			- A reverse proxy retrieves resources from one or more servers on behalf of a client. These resources are then returned to the client, appearing as if they originated from the proxy server itself, thus anonymizing the server.
			- A reverse proxy, just like a forward proxy, can be used for caching, load balancing, or routing requests to the appropriate servers.
			- ![](https://lwfiles.mycourse.app/systemdesign-public/2e33bf42fbcd7ac05a7031f39475e201.png)
		-
			-
	- ### SQL vs. NoSQL
	  collapsed:: true
		- Relational databases are structured and have predefined schemas like phone books that store phone numbers and addresses.
		- Non-relational databases are unstructured, distributed, and have a dynamic schema like file folders that hold everything from a person’s address and phone number to their Facebook ‘likes’ and online shopping preferences.
		- **SQL**
			- Relational databases store data in rows and columns. Each row contains all the information about one entity and each column contains all the separate data points.
			- Some of the most popular relational databases are MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB.
		- **NoSQL**
			- **Key-Value Stores**: Redis, Voldemort, and Dynamo
			- **Document Databases**: CouchDB and MongoDB
			- **Wide-Column Databases**
				- In columnar databases we have column families, which are containers for rows.
				- Unlike relational databases, we don’t need to know all the columns up front and each row doesn’t have to have the same number of columns.
				- Columnar databases are best suited for analyzing large datasets - big names include Cassandra and HBase.
			- **Graph Databases**
				- Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities).
				- Examples of graph database include Neo4J and InfiniteGraph.
		- **High level differences between SQL and NoSQL**
			- **Storage**
			- **Schema**
			- **Querying**
			- **Scalability**
				- In most common situations, SQL databases are vertically scalable, NoSQL databases are horizontally scalable.
			- **Reliability or ACID Compliancy (Atomicity, Consistency, Isolation, Durability)**
				- The vast majority of relational databases are ACID compliant.
				- Most of the NoSQL solutions sacrifice ACID compliance for performance and scalability.
		- **Which one to use?**
			- When it comes to database technology, there’s no one-size-fits-all solution.
			- Many businesses rely on both relational and non-relational databases for different needs
			- **Reasons to use SQL database**
				- We need to ensure ACID compliance.
				- Your data is structured and unchanging.
			- **Reasons to use NoSQL database**
				- Storing large volumes of data that often have little to no structure.
				- Making the most of cloud computing and storage.
				- Rapid development.
	- ### CAP Theorem
	  collapsed:: true
		- CAP theorem states that it is **impossible** for a distributed system to simultaneously provide all three of the following desirable properties
			- **Consistency ( C ):** All nodes see the same data at the same time. This means users can read or write from/to any node in the system and will receive the same data. It is equivalent to having a single up-to-date copy of the data.
			- **Availability ( A ):** Availability means every request received by a non-failing node in the system must result in a response. Even when severe network failures occur, every request must terminate. In simple terms, availability refers to a system’s ability to remain accessible even if one or more nodes in the system go down.
			- **Partition tolerance ( P ):** A partition is a communication break (or a network failure) between any two nodes in the system, i.e., both nodes are up but cannot communicate with each other. A partition-tolerant system continues to operate even if there are partitions in the system. Such a system can sustain any network failure that does not result in the failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages.
			- **In the presence of a network partition, a distributed system must choose either Consistency or Availability.**
			- ![](https://lwfiles.mycourse.app/systemdesign-public/2218b6b152cc5cb134ee0489d1f0c9a2.png)
		-
	- ### Consistent Hashing
	  collapsed:: true
		- David Karger et al. first introduced Consistent Hashing in their [1997 paper](https://dl.acm.org/doi/10.1145/258533.258660) and suggested its use in distributed caching.
		- The act of distributing data across a set of nodes is called data partitioning.
			- How do we know on which node a particular piece of data will be stored?
			- When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Additionally, how can we minimize data movement when nodes join or leave?
		- Consistent Hashing maps data to physical nodes and ensures that **only a small set of keys move when servers are added or removed.**
		- Vnodes are **randomly distributed** across the cluster and are generally **non-contiguous** so that no two neighboring Vnodes are assigned to the same physical node or rack.
		- **Dynamo** and **Cassandra** use Consistent Hashing to distribute their data across nodes.
	- ### Long-Polling vs WebSockets vs Server-Sent Events
	  collapsed:: true
		- **HTTP Long-Polling**
			- With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately. That’s why this technique is sometimes referred to as a “Hanging GET”.
		- **WebSockets**
			- WebSocket provides [Full duplex](https://en.wikipedia.org/wiki/Duplex_(telecommunications)#Full_duplex) communication channels over a single TCP connection.
		- **Server-Sent Events (SSEs)**
			- Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. If the client wants to send data to the server, it would require the use of another technology/protocol to do so.
	- ### Bloom Filters
	  collapsed:: true
		- The Bloom filter data structure tells whether an element **may be in a set, or definitely is not**.
		-
		-
	- ### Quorum
		- Once a system decides to maintain multiple copies of data, another problem arises: how to make sure that all replicas are consistent?
		- In a distributed environment, a quorum is the minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success.
		- **What value should we choose for a quorum?** More than half of the number of nodes in the cluster: (*N*/2+1) where *N* is the total number of nodes in the cluster
		- Quorum is achieved when nodes follow the below protocol: *R*+*W*>*N*
			- *N* = nodes in the quorum group
			  *W* = minimum write nodes
			  *R* = minimum read nodes
		- If a distributed system follows R+W>N rule, then every read will see at least one copy of the latest value written.
			- a common configuration could be (N=3, W=2, R=2) to ensure strong consistency
			- (N=3, W=1, R=3): fast write, slow read, not very durable
			- (N=3, W=3, R=1): slow write, fast read, durable
			- R=1 and W=N ⇒ full replication (write-all, read-one): undesirable when servers can be unavailable because writes are not guaranteed to complete.
			- B when 1<r<w<n, because reads are more frequent than writes in most applications.est performance (throughput/availability)