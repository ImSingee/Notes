alias:: DDIA/ch6
- [link](https://github.com/Vonng/ddia/blob/master/ch6.md)
-
- 对于非常大的数据集，或非常高的吞吐量，仅仅进行复制是不够的：我们需要将数据进行 **分区（partitions）**，也称为 **分片（sharding）**
-
- 分区主要是为了 **可伸缩性**。不同的分区可以放在不共享集群中的不同节点上；因此，大数据集可以分布在多个磁盘上，并且查询负载可以分布在多个处理器上
-
- 对于在单个分区上运行的查询，每个节点可以独立执行对自己的查询，因此可以通过添加更多的节点来扩大查询吞吐量。大型，复杂的查询可能会跨越多个节点并行处理，尽管这也带来了新的困难。
-
## 分区与复制
	- 分区通常与复制结合使用，使得每个分区的副本存储在多个节点上。这意味着，即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。
		- **通常情况下，每条数据（每条记录，每行或每个文档）属于且仅属于一个分区**
	- 一个节点可能存储多个分区
## 键值数据的分区
	- 分区目标是将数据和查询负载均匀分布在各个节点上。如果每个节点**公平**分享数据和负载，那么*理论上* 10 个节点应该能够处理 10 倍的数据量和 10 倍的单个节点的读写吞吐量（暂时忽略复制）。
		- 如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为 **偏斜（skew）**
			- 数据偏斜的存在使分区效率下降很多。
			- 不均衡导致的高负载的分区被称为 **热点（hot spot）**
	- 避免热点最简单的方法是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点。
	- ### 根据键的范围分区
		- 一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值）
			- 如果知道范围之间的边界，则可以轻松确定哪个分区包含某个值
		- **键的范围不一定均匀分布，因为数据也很可能不均匀分布**
			- 为了均匀分配数据，分区边界需要依据数据调整
			- 分区边界可以由管理员手动选择，也可以由数据库自动选择
		- 在每个分区中，我们可以按照一定的顺序保存键
			- [SSTables 和 LSM 树]([[DDIA/ch3]])
			- 好处：进行范围扫描非常简单，你可以将键作为联合索引来处理，以便在一次查询中获取多个相关记录
			- Key Range 分区的缺点是某些特定的访问模式会导致热点。
				- 如果主键是时间戳，则分区对应于时间范围，例如，给每天分配一个分区。 不幸的是，由于我们在测量发生时将数据从传感器写入数据库，因此所有写入操作都会转到同一个分区（即今天的分区），这样分区可能会因写入而过载，而其他分区则处于空闲状态
			- 为了避免传感器数据库中的这个问题，需要使用除了时间戳以外的其他东西作为主键的第一个部分。
	- ### 根据键的散列分区
		- 由于偏斜和热点的风险，许多分布式数据存储使用散列函数来确定给定键的分区。
		- 一个*好的*散列函数可以将偏斜的数据均匀分布。
		- 一旦你有一个合适的键散列函数，你可以为每个分区分配一个散列范围（而不是键的范围），每个通过哈希散列落在分区范围内的键将被存储在该分区中
			- 这种技术擅长在分区之间公平地分配键。分区边界可以是均匀间隔的，也可以是伪随机选择的（在这种情况下，该技术有时也被称为 **一致性哈希**，即 consistent hashing）。
			- 它使用随机选择的 **分区边界（partition boundaries）** 来避免中央控制或分布式共识的需要。
			- 请注意，这里的一致性与复制一致性或 ACID 一致性无关，而只是描述了一种再平衡（rebalancing）的特定方法。
			- 这种特殊的方法对于数据库实际上并不是很好，所以在实际中很少使用
			- 因为有可能产生混淆，所以最好避免使用一致性哈希这个术语，而只是把它称为 **散列分区（hash partitioning）**。
		- 不幸的是，通过使用键散列进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力
			- 折衷的策略：复合主键
				- 第一列进行散列哈希，剩下的范围存储
				- 如果第一列已经指定了固定值，则可以对该键的其他列执行有效的范围扫描
	- ### 负载偏斜与热点消除
		- 哈希分区可以帮助减少热点。但是，它不能完全避免它们
			- 在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区
		- 场景
			- 在社交媒体网站上，一个拥有数百万追随者的名人用户在做某事时可能会引发一场风暴。这个事件可能导致同一个键的大量写入（键可能是名人的用户 ID，或者人们正在评论的动作的 ID）。哈希策略不起作用，因为两个相同 ID 的哈希值仍然是相同的。
		- *大多数数据系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜*
		- 例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为 100 种不同的主键，从而存储在不同的分区中。
## 分区与次级索引
	- 次级索引通常并不能唯一地标识记录，而是一种搜索记录中出现特定值的方式
	- 次级索引的问题是它们不能整齐地映射到分区。有两种用次级索引对数据库进行分区的方法：**基于文档的分区（document-based）** 和 **基于关键词（term-based）的分区**。
	- ### 基于文档的次级索引进行分区
		- 每个分区是完全独立的：每个分区维护自己的次级索引，仅覆盖该分区中的文档。它不关心存储在其他分区的数据。
			- 无论何时你需要写入数据库（添加，删除或更新文档），只需处理包含你正在编写的文档 ID 的分区即可
		- **文档分区索引** 也被称为 **本地索引**
		- 如果要搜索，则需要将查询发送到所有分区，并合并所有返回的结果
			- 这种查询分区数据库的方法有时被称为 **分散 / 聚集（scatter/gather）**
			- 可能会使次级索引上的读取查询相当昂贵。即使并行查询分区，分散 / 聚集也容易导致尾部延迟放大
	- ### 基于关键词(Term)的次级索引进行分区
		- 我们可以构建一个覆盖所有分区数据的 **全局索引**，而不是给每个分区创建自己的次级索引（本地索引）。但是，我们不能只把这个索引存储在一个节点上，因为它可能会成为瓶颈，违背了分区的目的。全局索引也必须进行分区，但可以采用与主键不同的分区方式。
		- ![](https://github.com/Vonng/ddia/raw/master/img/fig6-5.png)
			- 来自所有分区的红色汽车在红色索引中，并且索引是分区的，首字母从 `a` 到 `r` 的颜色在分区 0 中，`s` 到 `z` 的在分区 1。汽车制造商的索引也与之类似（分区边界在 `f` 和 `h` 之间）。
		- 我们将这种索引称为 **关键词分区（term-partitioned）**，因为我们寻找的关键词决定了索引的分区方式。
		- 关键词分区的全局索引优于文档分区索引的地方点是它可以使读取更有效率：不需要 **分散 / 收集** 所有分区，客户端只需要向包含关键词的分区发出请求。
		- 全局索引的缺点在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上） 。
		- 理想情况下，索引总是最新的，写入数据库的每个文档都会立即反映在索引中。但是，在关键词分区索引中，这需要跨分区的分布式事务，并不是所有数据库都支持
		- 在实践中，对全局次级索引的更新通常是 **异步** 的（也就是说，如果在写入之后不久读取索引，刚才所做的更改可能尚未反映在索引中）
## 分区再平衡
	- 随着时间的推移，数据库会有各种变化而**需要数据和请求从一个节点移动到另一个节点**。
	- 将负载从集群中的一个节点向另一个节点移动的过程称为 **再平衡（rebalancing）**。
	- 无论使用哪种分区方案，再平衡通常都要满足一些最低要求：
		- 再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共享。
		- 再平衡发生时，数据库应该继续接受读取和写入。
		- 节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘 I/O 负载。
	- ### 再平衡策略
		- #### 反面教材：hash mod N
			- 模 N 方法的问题是，如果节点数量 N 发生变化，大多数键将需要从一个节点移动到另一个节点
			- 这使得再平衡的成本过高
		- #### 固定数量的分区
			- 有一个相当简单的解决方案：创建比节点更多的分区，并为每个节点分配多个分区
			- 如果一个节点被添加到集群中，新节点可以从当前每个节点中 **窃取** 一些分区，直到分区再次公平分配
			- 只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变的是分区所在的节点。这种变更并不是即时的 — 在网络上传输大量的数据需要一些时间 — 所以在传输过程中，原有分区仍然会接受读写操作。
		- #### 动态分区
			- 按键的范围进行分区的数据库（如 HBase 和 RethinkDB）会动态创建分区
				- 对于使用键范围分区的数据库，具有固定边界的固定数量的分区将非常不便：如果出现边界错误，则可能会导致一个分区中的所有数据或者其他分区中的所有数据为空。手动重新配置分区边界将非常繁琐。
			- 当分区增长到超过配置的大小时，会被分成两个分区，每个分区约占一半的数据
			- 与之相反，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。
			- 动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，所以开销很小；如果有大量的数据，每个分区的大小被限制在一个可配置的最大值
		- #### 按节点比例分区
			- 通过动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，**分区的数量都与节点的数量无关**。
			- 第三种方法是**使分区数与节点数成正比** —— 每个节点具有固定数量的分区
				- 每个分区的大小与数据集大小成比例地增长，而节点数量保持不变
				- 当增加节点数时，分区将再次变小
	- ### 运维：手动还是自动再平衡
		- 在全自动再平衡（系统自动决定何时将分区从一个节点移动到另一个节点，无须人工干预）和完全手动（分区指派给节点由管理员明确配置，仅在管理员明确重新配置时才会更改）之间有一个权衡
			- Couchbase、Riak 和 Voldemort 会自动生成建议的分区分配，但需要管理员提交才能生效。
		- 全自动再平衡可以很方便，因为正常维护的操作工作较少。然而，**它可能是不可预测的**。
			- 再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个节点。如果没有做好，这个过程可能会使网络或节点负载过重，降低其他请求的性能。
			- 这种自动化与自动故障检测相结合可能十分危险。例如，假设一个节点过载，并且对请求的响应暂时很慢。其他节点得出结论：过载的节点已经死亡，并自动重新平衡集群，使负载离开它。这会对已经超负荷的节点，其他节点和网络造成额外的负载，从而使情况变得更糟，并可能导致级联失败。
		- **再平衡的过程中有人参与是一件好事。这比全自动的过程慢，但可以帮助防止运维意外。**
## 请求路由
	- 当客户想要发出请求时，如何知道要连接哪个节点？
	- 这个问题可以概括为 **服务发现（service discovery）** ，它不仅限于数据库。任何可通过网络访问的软件都有这个问题，特别是如果它的目标是高可用性（在多台机器上运行冗余配置）。
	- 概括来说，这个问题有几种不同的方案
		- 允许客户联系任何节点（例如，通过 **循环策略的负载均衡**，即 Round-Robin Load Balancer）。如果该节点恰巧拥有请求的分区，则它可以直接处理该请求；否则，它将请求转发到适当的节点，接收回复并传递给客户端。
			- **流言协议（gossip protocol）** 传播集群状态的变化：请求可以发送到任意节点，该节点会转发到包含所请求的分区的适当节点
		- 首先将所有来自客户端的请求发送到路由层，它决定了应该处理请求的节点，并相应地转发。此路由层本身不处理任何请求；它仅负责分区的负载均衡。
		- 要求客户端知道分区和节点的分配。在这种情况下，客户端可以直接连接到适当的节点，而不需要任何中介。
			- **许多分布式数据系统都依赖于一个独立的协调服务**
				- 比如 ZooKeeper 来跟踪集群元数据
	- ### 执行并行查询
		- 到目前为止，我们只关注读取或写入单个键的非常简单的查询（加上基于文档分区的次级索引场景下的分散 / 聚集查询）。这也是大多数 NoSQL 分布式数据存储所支持的访问层级。
		- 然而，通常用于分析的 **大规模并行处理（MPP, Massively parallel processing）** 关系型数据库产品在其支持的查询类型方面要复杂得多。一个典型的数据仓库查询包含多个连接，过滤，分组和聚合操作。 MPP 查询优化器将这个复杂的查询分解成许多执行阶段和分区，其中许多可以在数据库集群的不同节点上并行执行。涉及扫描大规模数据集的查询特别受益于这种并行执行。
## 本章小结
	- 两种主要的分区方法
		- 键范围分区
		- 散列分区
		- <两种方法搭配使用也是可行的，例如使用复合主键：使用键的一部分来标识分区，而使用另一部分作为排序顺序。>
	- 分区和次级索引之间的相互作用。次级索引也需要分区
		- 基于文档分区（本地索引）
		- 基于关键词分区（全局索引）
	- 将查询路由到适当的分区的技术
	-
-