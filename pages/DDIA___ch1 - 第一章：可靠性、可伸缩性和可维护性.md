alias:: DDIA/ch1

- [link](https://github.com/Vonng/ddia/blob/master/ch1.md)
-
- 现今很多应用程序都是 **数据密集型（data-intensive）** 的，而非 **计算密集型（compute-intensive）** 的。因此 CPU 很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。
-
- 数据密集型应用通常由标准组件构建而成，标准组件提供了很多通用的功能；例如，许多应用程序都需要：
	- 存储数据，以便自己或其他应用程序之后能再次找到 （*数据库，即 databases*）
	- 记住开销昂贵操作的结果，加快读取速度（*缓存，即 caches*）
	- 允许用户按关键字搜索数据，或以各种方式对数据进行过滤（*搜索索引，即 search indexes*）
	- 向其他进程发送消息，进行异步处理（*流处理，即 stream processing*）
	- 定期处理累积的大批量数据（*批处理，即 batch processing*）
- 如果这些功能听上去平淡无奇，那是因为这些 **数据系统（data system）** 是非常成功的抽象：我们一直不假思索地使用它们并习以为常。绝大多数工程师不会幻想从零开始编写存储引擎，因为在开发应用时，数据库已经是足够完美的工具了。
-
- 本章将从我们所要实现的基础目标开始：**可靠、可伸缩、可维护**的数据系统。我们将澄清这些词语的含义，概述考量这些目标的方法。并回顾一些后续章节所需的基础知识。
	- 可靠性（Reliability）：系统在 **困境**（adversity，比如硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功能，并能达到期望的性能水准）。
	- 可伸缩性（Scalability）：有合理的办法应对系统的增长（数据量、流量、复杂性）
	- 可维护性（Maintainability）：许多不同的人（工程师、运维）在不同的生命周期，都能高效地在系统上工作（使系统保持现有行为，并适应新的应用场景）
- 人们经常追求这些词汇，却没有清楚理解它们到底意味着什么。为了工程的严谨性，本章的剩余部分将探讨可靠性、可伸缩性和可维护性的含义。为实现这些目标而使用的各种技术，架构和算法将在后续的章节中研究。
-
## 可靠性
	- 造成错误的原因叫做 **故障（fault）**，能预料并应对故障的系统特性可称为 **容错（fault-tolerant）** 或 **韧性（resilient）**。
		- “**容错**” 一词可能会产生误导，因为它暗示着系统可以容忍所有可能的错误，但在实际中这是不可能的（例如，世界毁灭）。
			- 在讨论容错时，只有谈论特定类型的错误才有意义。
		- **故障（fault）** 不同于 **失效（failure）**
			- **故障** 通常定义为系统的一部分状态偏离其标准
			- 而 **失效** 则是系统作为一个整体停止向用户提供服务
			- 故障的概率不可能降到零，因此最好设计容错机制以防因 **故障** 而导致 **失效**
		- 反直觉的是，在这类容错系统中，通过故意触发来 **提高** 故障率是有意义的
			- 例如：在没有警告的情况下随机地杀死单个进程。
			- 许多高危漏洞实际上是由糟糕的错误处理导致的，因此我们可以通过故意引发故障来确保容错机制不断运行并接受考验，从而提高故障自然发生时系统能正确处理的信心。
			- Netflix 公司的 *Chaos Monkey*就是这种方法的一个例子。
		- 比起 **阻止错误（prevent error）**，我们通常更倾向于 **容忍错误**
			- 但也有 **预防胜于治疗** 的情况（比如不存在治疗方法时）。安全问题就属于这种情况。
			- 但本书主要讨论的是可以恢复的故障
	- ### 硬件故障
	  collapsed:: true
		- 当想到系统失效的原因时，**硬件故障（hardware faults）** 总会第一个进入脑海
			- 硬盘崩溃、内存出错、机房断电、有人拔错网线……
			- 一旦你拥有很多机器，这些事情 **总** 会发生！
				- 据报道称，硬盘的 **平均无故障时间（MTTF, mean time to failure）** 约为 10 到 50 年。因此从数学期望上讲，在拥有 10000 个磁盘的存储集群上，平均每天会有 1 个磁盘出故障。
		- 为了减少系统的故障率，第一反应通常都是增加单个硬件的冗余度
			- 磁盘 RAID
			- 双路电源
			- 热插拔 CPU
			- 数据中心可能有电池和柴油发电机作为后备电源
			- 某个组件挂掉时冗余组件可以立刻接管
			- **这种方法虽然不能完全防止由硬件问题导致的系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。**
				- 硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见
				- 但是随着数据量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增加硬件故障率
		- 云平台
			- 在类似亚马逊 AWS（Amazon Web Services）的一些云服务平台上，虚拟机实例不可用却没有任何警告也是很常见的
			- 云平台的设计就是优先考虑 **灵活性（flexibility）** 和 **弹性（elasticity）**，而不是单机可靠性。
		- 如果在硬件冗余的基础上**进一步引入软件容错机制**，那么系统在容忍整个（单台）机器故障的道路上就更进一步了。
			- 这样的系统也有运维上的便利
				- 例如：如果需要重启机器（例如应用操作系统安全补丁），单服务器系统就需要计划停机。而允许机器失效的系统则可以一次修复一个节点，无需整个系统停机。
	- ### 软件错误
	  collapsed:: true
		- 我们通常认为硬件故障是随机的、相互独立的
			- 一台机器的磁盘失效并不意味着另一台机器的磁盘也会失效。虽然大量硬件组件之间可能存在微弱的相关性（例如服务器机架的温度等共同的原因），但同时发生故障也是极为罕见的
		- 另一类错误是内部的 **系统性错误（systematic error）**
			- 这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的 **系统失效**
		- 例
			- 接受特定的错误输入，便导致所有应用服务器实例崩溃的 BUG。例如 2012 年 6 月 30 日的闰秒，由于 Linux 内核中的一个错误，许多应用同时挂掉了。
			- 失控进程会用尽一些共享资源，包括 CPU 时间、内存、磁盘空间或网络带宽。
			- 系统依赖的服务变慢，没有响应，或者开始返回错误的响应。
			- 级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障。
		- 导致这类软件故障的 BUG 通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味着软件对其环境做出了某种假设 —— 虽然这种假设通常来说是正确的，但由于某种原因最后不再成立了
		- 软件中的系统性故障没有速效药，但我们还是有很多小办法
			- 仔细考虑系统中的假设和交互；
			- 彻底的测试；
			- 进程隔离；
			- 允许进程崩溃并重启；
			- 测量、监控并分析生产环境中的系统行为
		- 如果系统能够提供一些保证（例如在一个消息队列中，进入与发出的消息数量相等），那么系统就可以在运行时不断自检，并在出现 **差异（discrepancy）** 时报警
	- ### 人为错误
	  collapsed:: true
		- 即使他们怀有最大的善意，人类也是不可靠的
			- 一项关于大型互联网服务的研究发现，运维配置错误是导致服务中断的首要原因，而硬件故障（服务器或网络）仅导致了 10-25% 的服务中断
		- 最好的系统会组合使用以下几种办法
			- 以最小化犯错机会的方式设计系统。例如，精心设计的抽象、API 和管理后台使做对事情更容易，搞砸事情更困难。但如果接口限制太多，人们就会忽略它们的好处而想办法绕开。很难正确把握这种微妙的平衡。
			- 将人们最容易犯错的地方与可能导致失效的地方 **解耦（decouple）**。特别是提供一个功能齐全的非生产环境 **沙箱（sandbox）**，使人们可以在不影响真实用户的情况下，使用真实数据安全地探索和实验。
			- 在各个层次进行彻底的测试，从单元测试、全系统集成测试到手动测试。自动化测试易于理解，已经被广泛使用，特别适合用来覆盖正常情况中少见的 **边缘场景（corner case）**。
			- 允许从人为错误中简单快速地恢复，以最大限度地减少失效情况带来的影响。 例如，快速回滚配置变更，分批发布新代码（以便任何意外错误只影响一小部分用户），并提供数据重算工具（以备旧的计算出错）。
			- 配置详细和明确的监控，比如性能指标和错误率。 在其他工程学科中这指的是 **遥测（telemetry）**（一旦火箭离开了地面，遥测技术对于跟踪发生的事情和理解失败是至关重要的）。监控可以向我们发出预警信号，并允许我们检查是否有任何地方违反了假设和约束。当出现问题时，指标数据对于问题诊断是非常宝贵的。
			- 良好的管理实践与充分的培训 —— 一个复杂而重要的方面，但超出了本书的范围。
## 可伸缩性
	- 系统今天能可靠运行，并不意味未来也能可靠运行。
	- 服务 **降级（degradation）** 的一个常见原因是负载增加
	- **可伸缩性（Scalability）** 是用来描述系统应对负载增长能力的术语
		- 这不是贴在系统上的一维标签：说 “X 可伸缩” 或 “Y 不可伸缩” 是没有任何意义的。
		- 相反，讨论可伸缩性意味着考虑诸如 “如果系统以特定方式增长，有什么选项可以应对增长？” 和 “如何增加计算资源来处理额外的负载？” 等问题。
	- ### 描述负载
		- 负载可以用一些称为 **负载参数（load parameters）** 的数字来描述。
			- 参数的最佳选择取决于系统架构
				- 它可能是每秒向 Web 服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西
	- ### 描述性能
		- 一旦系统的负载被描述好，就可以研究当负载增加会发生什么
			- 增加负载参数并保持系统资源（CPU、内存、网络带宽等）不变时，系统性能将受到什么影响？
			- 增加负载参数并希望保持性能不变时，需要增加多少系统资源？
		- 如何描述系统性能
			- 批处理系统：通常关心的是 **吞吐量（throughput）**
				- 即每秒可以处理的记录数量，或者在特定规模数据集上运行作业的总时间
			- 对于在线系统，通常更重要的是服务的 **响应时间（response time）**
				- 即客户端发送请求到接收响应之间的时间。
				- **延迟和响应时间**
					- **延迟（latency）** 和 **响应时间（response time）** 经常用作同义词，但实际上它们并不一样。
					- 响应时间是客户所看到的，除了实际处理请求的时间（ **服务时间（service time）** ）之外，还包括网络延迟和排队延迟。
					- 延迟是某个请求等待处理的 **持续时长**，在此期间它处于 **休眠（latent）** 状态，并等待服务
			- 即使不断重复发送同样的请求，每次得到的响应时间也都会略有不同。现实世界的系统会处理各式各样的请求，响应时间可能会有很大差异。因此我们需要将响应时间视为一个可以测量的数值 **分布（distribution）**，而不是单个数值。
			  collapsed:: true
				- 平均响应时间
					- 严格来讲 “平均” 一词并不指代任何特定公式，但实际上它通常被理解为 **算术平均值（arithmetic mean）**：给定 n 个值，加起来除以 n ）。
					- 然而如果你想知道 “**典型（typical）**” 响应时间，那么平均值并不是一个非常好的指标，因为它不能告诉你有多少用户实际上经历了这个延迟。
				- 百分位点（percentiles）
					- 如果想知道典型场景下用户需要等待多长时间，那么中位数是一个好的度量标准
					- 为了弄清异常值有多糟糕，可以看看更高的百分位点，例如第 95、99 和 99.9 百分位点（缩写为 p95，p99 和 p999）。它们意味着 95%、99% 或 99.9% 的请求响应时间要比该阈值快
						- 响应时间的高百分位点（也称为 **尾部延迟**，即 **tail latencies**）非常重要，因为它们直接影响用户的服务体验
						- 百分位点通常用于 **服务级别目标（SLO, service level objectives）** 和 **服务级别协议（SLA, service level agreements）**，即定义服务预期性能和可用性的合同。 SLA 可能会声明，如果服务响应时间的中位数小于 200 毫秒，且 99.9 百分位点低于 1 秒，则认为服务工作正常（如果响应时间更长，就认为服务不达标）。这些指标为客户设定了期望值，并允许客户在 SLA 未达标的情况下要求退款。
						- **排队延迟（queueing delay）** 通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其 CPU 核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为 **头部阻塞（head-of-line blocking）** 。即使后续请求在服务器上处理的非常迅速，由于需要等待先前请求完成，客户端最终看到的是缓慢的总体响应时间。因为存在这种效应，测量客户端的响应时间非常重要。
						- **实践中的百分位点**
							- 在**多重调用**的后端服务里，高百分位数变得特别重要。
							- 即使并行调用，最终用户请求仍然需要等待最慢的并行调用完成。
	- ### 应对负载的方法
		- **纵向伸缩**（scaling up，也称为垂直伸缩，即 vertical scaling，转向更强大的机器）和 **横向伸缩**（scaling out，也称为水平伸缩，即 horizontal scaling，将负载分布到多台小机器上）
		- 跨多台机器分配负载也称为 “**无共享（shared-nothing）**” 架构。可以在单台机器上运行的系统通常更简单
		- 有些系统是 **弹性（elastic）** 的，这意味着可以在检测到负载增加时自动增加计算资源，而其他系统则是手动伸缩（人工分析容量并决定向系统添加更多的机器）。如果负载 **极难预测（highly unpredictable）**，则弹性系统可能很有用，但手动伸缩系统更简单，并且意外操作可能会更少
		- 跨多台机器部署 **无状态服务（stateless services）** 非常简单，但将带状态的数据系统从单节点变为分布式配置则可能引入许多额外复杂度。出于这个原因，常识告诉我们应该将数据库放在单个节点上（纵向伸缩），直到伸缩成本或可用性需求迫使其改为分布式。
		- 一个良好适配应用的可伸缩架构，是围绕着 **假设（assumption）** 建立的：哪些操作是常见的？哪些操作是罕见的？这就是所谓负载参数。如果假设最终是错误的，那么为伸缩所做的工程投入就白费了，最糟糕的是适得其反。在早期创业公司或非正式产品中，通常支持产品快速迭代的能力，要比可伸缩至未来的假想负载要重要的多。
## 可维护性
	- 软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段
		- 包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债和添加新的功能。
	- 在设计之初就尽量考虑尽可能减少维护期间的痛苦，从而避免自己的软件系统变成**遗留（legacy）系统**
		- 关注软件系统的三个设计原则
			- **可操作性（Operability）**便于运维团队保持系统平稳运行。
			- **简单性（Simplicity）**从系统中消除尽可能多的 **复杂度（complexity）**，使新工程师也能轻松理解系统（注意这和用户接口的简单性不一样）。
			- **可演化性（evolvability）**使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为 **可扩展性（extensibility）**、**可修改性（modifiability）** 或 **可塑性（plasticity）**。
		- 和之前提到的可靠性、可伸缩性一样，实现这些目标也没有简单的解决方案
	- ### 可操作性：人生苦短，关爱运维
		- 尽管运维的某些方面可以，而且应该是自动化的，但在最初建立正确运作的自动化机制仍然取决于人。
		- 良好的可操作性意味着更轻松的日常工作，进而运维团队能专注于高价值的事情。数据系统可以通过各种方式使日常任务更轻松：
			- 通过**良好的监控**，提供对系统内部状态和运行时行为的 **可见性（visibility）**。
			- **为自动化提供良好支持**，将系统与标准化工具相集成。
			- **避免依赖单台机器**（在整个系统继续不间断运行的情况下允许机器停机维护）。
			- 提供良好的文档和易于理解的操作模型（“如果做 X，会发生 Y”）。
			- 提供良好的默认行为，但需要时也允许管理员自由覆盖默认值。
			- 有条件时进行自我修复，但需要时也允许管理员手动控制系统状态。
			- 行为可预测，最大限度减少意外。
	- ### 简单性：管理复杂度
		- **复杂度（complexity）** 有各种可能的症状
			- 例如：状态空间激增、模块间紧密耦合、纠结的依赖关系、不一致的命名和术语、解决性能问题的 Hack、需要绕开的特例等等
		- 简化系统并不一定意味着减少功能；它也可以意味着消除 **额外的（accidental）** 的复杂度
			- **额外复杂度** 定义为：由具体实现中涌现，而非（从用户视角看，系统所解决的）问题本身固有的复杂度。
		- 用于消除 **额外复杂度** 的最好工具之一是 **抽象（abstraction）**。
			- 一个好的抽象可以将大量实现细节隐藏在一个干净，简单易懂的外观下面。一个好的抽象也可以广泛用于各类不同应用。
	- ### 可演化性：拥抱变化
		- **敏捷（agile）** 工作模式为适应变化提供了一个框架。敏捷社区还开发了对在频繁变化的环境中开发软件很有帮助的技术工具和模式，如 **测试驱动开发（TDD, test-driven development）** 和 **重构（refactoring）** 。
		- 修改数据系统并使其适应不断变化需求的容易程度，是与 **简单性** 和 **抽象性** 密切相关的：简单易懂的系统通常比复杂系统更容易修改。但由于这是一个非常重要的概念，我们将用一个不同的词来指代数据系统层面的敏捷性： **可演化性（evolvability）**
## 本章小结
	- 本章探讨了一些关于数据密集型应用的基本思考方式。这些原则将指导我们阅读本书的其余部分，那里将会深入技术细节。
	- 一个应用必须满足各种需求才称得上有用。有一些 **功能需求**（functional requirements，即它应该做什么，比如允许以各种方式存储，检索，搜索和处理数据）以及一些 **非功能性需求**（nonfunctional，即通用属性，例如安全性、可靠性、合规性、可伸缩性、兼容性和可维护性）。在本章详细讨论了可靠性，可伸缩性和可维护性。
	- **可靠性（Reliability）** 意味着即使发生故障，系统也能正常工作。故障可能发生在硬件（通常是随机的和不相关的）、软件（通常是系统性的 Bug，很难处理）和人类（不可避免地时不时出错）。 **容错技术** 可以对终端用户隐藏某些类型的故障。
	- **可伸缩性（Scalability）** 意味着即使在负载增加的情况下也有保持性能的策略。为了讨论可伸缩性，我们首先需要定量描述负载和性能的方法。我们简要了解了推特主页时间线的例子，介绍描述负载的方法，并将响应时间百分位点作为衡量性能的一种方式。在可伸缩的系统中可以添加 **处理容量（processing capacity）** 以在高负载下保持可靠。
	- **可维护性（Maintainability）** 有许多方面，但实质上是关于工程师和运维团队的生活质量的。良好的抽象可以帮助降低复杂度，并使系统易于修改和适应新的应用场景。良好的可操作性意味着对系统的健康状态具有良好的可见性，并拥有有效的管理手段。
	- 不幸的是，使应用可靠、可伸缩或可维护并不容易。但是某些模式和技术会不断重新出现在不同的应用中。在接下来的几章中，我们将看到一些数据系统的例子，并分析它们如何实现这些目标。
	-
	-