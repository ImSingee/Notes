title:: Readwise/Highlights/Superalignment Fast Grants
author:: [[openai.com]]
full-title:: Superalignment Fast Grants
category:: #articles
url:: https://openai.com/blog/superalignment-fast-grants
tags:: #[[go inbox]] #[[inoreader]] #[[openai]] #[[read]]  
![](https://openai.com/favicon.png)
- reinforcement learning from human feedback (RLHF) ([View Highlight](https://read.readwise.io/read/01hhntnt7zkdfrz2je65sb1k7s)) #Highlight #[[2023-12-15]]
- Superhuman AI systems will be capable of complex and creative behaviors that humans cannot fully understand. For example, if a superhuman model generates a million lines of extremely complicated code, humans will not be able to reliably evaluate whether the code is safe or dangerous to execute. Existing alignment techniques like RLHF that rely on human supervision may no longer be sufficient. **This leads to the fundamental challenge: how can humans steer and trust AI systems much smarter than them?** ([View Highlight](https://read.readwise.io/read/01hhntpgr7g95b8440rpe7nhtq)) #Highlight #[[2023-12-15]]