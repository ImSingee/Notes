title:: Readwise/Highlights/这就是ChatGPT
author:: [[斯蒂芬·沃尔弗拉姆]]
full-title:: 这就是ChatGPT
category:: #books


![](https://cdn.weread.qq.com/weread/cover/24/cpplatform_4cn8w4tmgzntjobg9ffeny/s_cpplatform_4cn8w4tmgzntjobg9ffeny1703648785.jpg)
None

- ChatGPT的基础是人工神经网络（本书中一般简称为神经网络或网络），后者最初是在20世纪40年代为了模拟理想化的大脑运作方式而发明的 #Highlight #[[2024-04-26]]
- GPT技术路线的一大核心理念，是用最简单的自回归生成架构来解决无监督学习问题，也就是利用无须人特意标注的原始数据，学习其中对世界的映射 #Highlight #[[2024-04-26]]
- 神经网络之所以很有用（人脑中的神经网络大概也如此），原因不仅在于它可以执行各种任务，还在于它可以通过逐步“根据样例训练”来学习执行这些任务。 #Highlight #[[2024-05-20]]
- 从来没有“无模型的模型”。你使用的任何模型都有某种特定的基本结构，以及用于拟合数据的一定数量的“旋钮”（也就是可以设置的参数）。ChatGPT使用了许多这样的“旋钮”—实际上有1750亿个。 #Highlight #[[2024-05-20]]
- 最佳思路是建立一个模型，让我们能够估计序列出现的概率—即使我们从未在已有的文本语料库中明确看到过这些序列。ChatGPT的核心正是所谓的“大语言模型”，后者已经被构建得能够很好地估计这些概率了。 #Highlight #[[2024-05-20]]
- 上图展示了，在仅有两个权重的情况下可能需要进行的最小化工作。但是事实证明，即使有更多的权重（ChatGPT使用了1750亿个权重），也仍然可以进行最小化，至少可以在某种程度上进行近似。实际上，“深度学习”在2012年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。 #Highlight #[[2024-05-20]]
- 有时候用神经网络解决复杂问题比解决简单问题更容易—这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。 #Highlight #[[2024-05-20]]
- 在典型情况下，有许多不同的权重集合可以使神经网络具有几乎相同的性能。在实际的神经网络训练中，通常会做出许多随机选择，导致产生一些“不同但等效”的解决方案 #Highlight #[[2024-05-20]]
- 我们能“从数学上”解释网络是如何做出区分的吗？并不能。它只是在“做神经网络要做的事”。但是事实证明，这通常与我们人类所做的区分相当吻合 #Highlight #[[2024-05-20]]
- 人类大脑有大约1000亿个神经元（神经细胞），每个神经元都能够产生电脉冲，最高可达每秒约1000次。这些神经元连接成复杂的网络，每个神经元都有树枝状的分支，从而能够向其他数千个神经元传递电信号。粗略地说，任意一个神经元在某个时刻是否产生电脉冲，取决于它从其他神经元接收到的电脉冲，而且神经元不同的连接方式会有不同的“权重”贡献。 #Highlight #[[2024-05-28]]
- 重点在于，已训练的神经网络能够对所展示的特定例子进行“泛化” #Highlight #[[2024-05-28]]
- 无论输入什么，神经网络都会生成一个答案。结果表明，它的做法相当符合人类的思维方式。正如上面所说的，这并不是我们可以“根据第一性原则推导”出来的事实。这只是一些经验性的发现，至少在某些领域是正确的。但这是神经网络有用的一个关键原因：它们以某种方式捕捉了“类似人类”的做事方式。 #Highlight #[[2024-05-28]]
- 在神经网络的早期发展阶段，人们倾向于认为应该“让神经网络做尽可能少的事”。例如，在将语音转换为文本时，人们认为应该先分析语音的音频，再将其分解为音素，等等。但是后来发现，（至少对于“类人任务”）最好的方法通常是尝试训练神经网络来“解决端到端的问题”，让它自己“发现”必要的中间特征、编码等。 #Highlight #[[2024-06-04]]
- 为特定的任务训练神经网络需要多少数据？根据第一性原则很难估计。使用“迁移学习”可以将已经在另一个神经网络中学习到的重要特征列表“迁移过来”，从而显著降低对数据规模的要求。但是，神经网络通常需要“看到很多样例”才能训练好。至少对于某些任务而言，神经网络学问中很重要的一点是，样例的重复可能超乎想象。事实上，不断地向神经网络展示所有的样例是一种标准策略。在每个“训练轮次”（training round或epoch）中，神经网络都会处于至少稍微不同的状态，而且向它“提醒”某个特定的样例对于它“记忆该样例”是有用的。（是的，这或许类似于重复在人类记忆中的有用性。） #Highlight #[[2024-06-04]]
- 神经网络的实际学习过程是怎样的呢？归根结底，核心在于确定哪些权重能够最好地捕捉给定的训练样例。有各种各样的详细选择和“超参数设置”（之所以这么叫，是因为权重也称为“参数”），可以用来调整如何进行学习。有不同的损失函数可以选择，如平方和、绝对值和，等等。有不同的损失最小化方法，如每一步在权重空间中移动多长的距离，等等。然后还有一些问题，比如“批量”(batch)展示多少个样例来获得要最小化的损失的连续估计。是的，我们可以（像在Wolfram语言中所做的一样）应用机器学习来自动化机器学习，并自动设置超参数等。 #Highlight #[[2024-06-04]]